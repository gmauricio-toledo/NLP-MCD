{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-MCD/blob/main/13-LLM_Prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVZgx_GECmn3"
      },
      "source": [
        "# Sentiment Analysis using LLMs\n",
        "\n",
        "En esta notebook realizaremos la tarea de An치lisis de Sentimientos usando un LLM de la librer칤a `transformers` de Hugging Face. Probaremos varios modelos y t칠cnicas para hacer la tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "53ry57Wkxkt6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mQAiAJp3EAT"
      },
      "outputs": [],
      "source": [
        "!gdown 18kGdlhOiQNS61wUK7uPbdquKL3XJrgzf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Bp5a22R3ITJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "imdb_df = pd.read_csv('IMDB.csv')\n",
        "display(imdb_df)\n",
        "\n",
        "y = LabelEncoder().fit_transform(imdb_df['sentiment'].values)\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(imdb_df['review'].values, y, test_size=0.2, random_state=642, stratify=y)\n",
        "X_train_raw, X_val_raw, y_train, y_val = train_test_split(X_train_raw, y_train, test_size=0.25, random_state=473, stratify=y_train)\n",
        "print(f\"Training set size: {len(X_train_raw)}\")\n",
        "print(f\"Validation set size: {len(X_val_raw)}\")\n",
        "print(f\"Test set size: {len(X_test_raw)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento\n",
        "\n",
        "En los modelos de lenguaje modernos como BERT y sus sucesores, el preprocesamiento tradicional del texto (como la eliminaci칩n de stopwords, lematizaci칩n o stemming) ya no es necesario ni recomendable.\n",
        "\n",
        "Estos modelos est치n dise침ados para entender el contexto y la estructura del lenguaje tal como aparece en el texto crudo, incluyendo palabras funcionales que aportan significado contextual.\n",
        "\n",
        "Sin embargo, s칤 es com칰n limpiar el texto de artefactos no ling칲칤sticos, como etiquetas HTML, c칩digos de escape, URLs, o caracteres especiales irrelevantes.\n",
        "\n",
        "En su lugar, el preprocesamiento se limita generalmente a la tokenizaci칩n mediante el tokenizador espec칤fico del modelo (por ejemplo, WordPiece para BERT), la adici칩n de tokens especiales ([CLS], [SEP]), y el relleno o truncamiento de secuencias para ajustarlas a una longitud fija.\n",
        "\n",
        "Conservar el texto original permite al modelo aprovechar al m치ximo su capacidad contextual y sem치ntica.\n",
        "\n"
      ],
      "metadata": {
        "id": "yZj6bVbXxmIC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWYIoPr8kZTM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import html\n",
        "\n",
        "def clean_text(text):\n",
        "    # Decodificar entidades HTML\n",
        "    text = html.unescape(text)\n",
        "    # Eliminar etiquetas HTML\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    # Normalizar espacios\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukq04-bXkhdu"
      },
      "outputs": [],
      "source": [
        "train_docs = [clean_text(doc) for doc in X_train_raw]\n",
        "test_docs = [clean_text(doc) for doc in X_test_raw]\n",
        "val_docs = [clean_text(doc) for doc in X_val_raw]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampleo"
      ],
      "metadata": {
        "id": "95bMIiqNxo_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMGCfdKc3Ut-"
      },
      "outputs": [],
      "source": [
        "num_training_docs = 300\n",
        "num_validation_docs = 1000\n",
        "\n",
        "sample_train_docs, _, sample_train_labels, _ = train_test_split(train_docs, y_train,\n",
        "                                                                train_size=num_training_docs,\n",
        "                                                                random_state=777,\n",
        "                                                                stratify=y_train)\n",
        "\n",
        "sample_val_docs, _, sample_val_labels, _ = train_test_split(val_docs, y_val,\n",
        "                                                            train_size=num_validation_docs,\n",
        "                                                            random_state=777,\n",
        "                                                            stratify=y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo"
      ],
      "metadata": {
        "id": "6umWYDMlxqx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probemos algunos modelos:\n",
        "\n",
        "* [Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct): Modelo de Microsoft con 3.8B par치metros, 128K tokens context length, vocabulario de 32064 tokens, entrenado en agosto/2024.\n",
        "* [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct): Modelo de Qwen con 1.54B par치metros, 32,768 tokens context length, Multilingual support for over 29 languages.\n",
        "\n",
        "Observa que ahora usamos la clase Clase `AutoModelForCausalLM` de HuggingFace Transformers, es para modelos de lenguaje generativo. Carga autom치ticamente la arquitectura correcta seg칰n el nombre del modelo.\n",
        "\n",
        "Tipos de tarea:\n",
        "* Generaci칩n de texto\n",
        "* Completaci칩n de prompts\n",
        "* Chatbots\n",
        "* Predicci칩n del siguiente token"
      ],
      "metadata": {
        "id": "dJQaaTa1vuj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCieeuYRP4e8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "# model_name = \"Qwen/Qwen-7B-Chat\"\n",
        "# model_name = \"mosaicml/mpt-7b\"\"\n",
        "# model_name = \"tiiuae/falcon-7b-instruct\"\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "# model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rqV0Bo7rH8A"
      },
      "source": [
        "Si usamos los documentos sin quitar stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkIzGrcRrCsO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "docs, _, labels, _ = train_test_split(sample_val_docs,\n",
        "                                      sample_val_labels,\n",
        "                                      train_size=200,\n",
        "                                      stratify=sample_val_labels,\n",
        "                                      random_state=707)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "xO7SJvePxtQm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97bs5ZhEChpi"
      },
      "source": [
        "### Zero shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Tkg4Lkirbo"
      },
      "source": [
        "Con GPU y 200 ejemplos, tarda alrededor de 1 minuto"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "ZeP0nnhUpK0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haremos este [prompt](https://claude.ai/share/3b52a333-a017-4de0-9970-201b824a2b52) de forma iterada, solamente sobre un n칰mero peque침o de ejemplos"
      ],
      "metadata": {
        "id": "MNKhG8Lkpljb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hi, what can you tell me about your self?\"\n",
        "messages = [\n",
        "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "    {'role': 'user', 'content': prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True)\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=16\n",
        ")\n",
        "generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "id": "2dX_hwwrsQNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgH5Y_Cz3dvC"
      },
      "outputs": [],
      "source": [
        "responses = []\n",
        "\n",
        "for k,sentence in enumerate(docs):\n",
        "    prompt = \"I want to perform a binary sentiment analysis task on the following text, determine if the sentiment is positive or negative. Respond only 'positive' or 'negative'. The text is: \" + sentence\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant performing binary sentiment analysis.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=16\n",
        "    )\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    print(f\"{k+1}/{num_training_docs}\")\n",
        "    responses.append(response)\n",
        "\n",
        "print(responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAJDmj-xCkgY"
      },
      "source": [
        "### Few shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLGdSP2Eq2sy"
      },
      "source": [
        "Obtenemos unos pocos documentos de ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akxFR6K3jFIH"
      },
      "outputs": [],
      "source": [
        "sample_docs = sample_val_docs[:3].copy()\n",
        "sample_labels = sample_val_labels[:3].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeMlf-QM-tnp"
      },
      "outputs": [],
      "source": [
        "print(sample_docs)\n",
        "print(sample_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "RFAZCoTFuKvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tarda alrededor de 7 minutos"
      ],
      "metadata": {
        "id": "8L--D9BewOZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyxCEn5rBOg3"
      },
      "outputs": [],
      "source": [
        "responses = []\n",
        "\n",
        "label_to_text = {0: \"negative\", 1: \"positive\"}\n",
        "\n",
        "for k, sentence in enumerate(docs):\n",
        "    prompt = \"\"\n",
        "    for label, text in zip(sample_labels[1:], sample_docs[1:]):\n",
        "        prompt += f\"\\n{text} // {label_to_text[label]}\\n\"\n",
        "    prompt += f\"\\n{sentence} // \"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant performing binary sentiment analysis. You must respond ONLY with 'positive' or 'negative', nothing else.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=3,  # Reducido ya que solo necesitamos una palabra\n",
        "        num_return_sequences=1,\n",
        "        do_sample=False,   # Para hacerlo m치s determinista\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Limpiar la respuesta para obtener solo \"positive\" o \"negative\"\n",
        "    response = response.strip().lower()\n",
        "    if \"positive\" in response:\n",
        "        response = \"positive\"\n",
        "    elif \"negative\" in response:\n",
        "        response = \"negative\"\n",
        "    else:\n",
        "        # Si no es claro, usar una respuesta por defecto\n",
        "        response = \"unknown\"\n",
        "\n",
        "    print(f\"{k+1}/{len(docs)}: {response}\")\n",
        "    responses.append(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluaci칩n"
      ],
      "metadata": {
        "id": "cdAqpVSQvlS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HPEauRVwvPE"
      },
      "outputs": [],
      "source": [
        "def encode(x):\n",
        "    if x.lower() == \"positive\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "y_pred = [encode(x) for x in responses]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kjo9dUJjOxI"
      },
      "source": [
        "Zero-shot:\n",
        "\n",
        "* Phi-3.5-mini-instruct: 94%\n",
        "* Qwen2.5 1.5B: ?\n",
        "\n",
        "Few shot:\n",
        "* Phi-3.5-mini-instruct: 91%\n",
        "* Qwen2.5 1.5B: ?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs), len(labels)"
      ],
      "metadata": {
        "id": "ZayvW83qrnMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8awgE6N0ykx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(labels, y_pred)}\")\n",
        "plt.figure()\n",
        "sns.heatmap(confusion_matrix(labels, y_pred), annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Explorations\n",
        "\n",
        "Est치 t칠cnica *Dynamic Zero-Shot Categorization* podr칤amos usarla en m치s tareas:\n",
        "* Topic Modeling\n",
        "* Information Extraction\n",
        "\n",
        "---\n",
        "\n",
        "游댮 Exploraciones adicionales\n",
        "* Explorar el efecto de los ejemplos para el few shot:\n",
        "    * La longitud de los textos de ejemplo\n",
        "    * El n칰mero de ejemplos\n",
        "* Explorar m치s LLM.\n",
        "* Explorar diferentes prompts.\n",
        "* Explorar el [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) de [*sentiment analysis*](https://huggingface.co/blog/sentiment-analysis-python) de HuggingFace, hay muchos [modelos](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment) para escoger.\n",
        "* Usar los embeddings generados por el modelo y aplicar algoritmos de ML.\n"
      ],
      "metadata": {
        "id": "LmheWTS7x15O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZcIj00Vj4dl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}