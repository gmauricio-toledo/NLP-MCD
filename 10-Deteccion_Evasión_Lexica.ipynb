{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ra--Tj6H1q1CzhpalacNEMxkX9704TGR",
      "authorship_tag": "ABX9TyMIS8Uzfyj94eVJutpqM7sT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-MCD/blob/main/10-Deteccion_Evasi%C3%B3n_Lexica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Detectando Evasión Léxica con Embeddings de Palabras</h1>\n",
        "\n",
        "En esta notebook exploramos cómo los embeddings de palabras, como los generados por Word2Vec o FastText, pueden utilizarse para detectar intentos de evasión léxica en textos, un fenómeno común en redes sociales donde se alteran palabras ofensivas o prohibidas mediante sustituciones de caracteres (por ejemplo, \"m4t4r\" en lugar de \"matar\") o el uso de sinónimos engañosos (como \"desvivir\"). Entrenamos un modelo de embeddings a partir de un corpus de subtítulos en español modificado artificialmente para incluir estas variantes, y evaluamos la capacidad del modelo para capturar la cercanía semántica entre las palabras originales y sus versiones disfrazadas mediante similitud coseno.\n",
        "\n",
        "\n",
        "Este enfoque permite ir más allá de los filtros basados únicamente en coincidencias exactas, ofreciendo una estrategia más robusta para la moderación de contenido, detección de spam, prevención de acoso en línea y otras aplicaciones donde el lenguaje se adapta intencionalmente para eludir sistemas de control."
      ],
      "metadata": {
        "id": "qyHWw-6PhfUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "FjVHXX8HYbB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus original\n",
        "\n",
        "El corpus original se encuentra en [HuggingFace](https://huggingface.co/datasets/Helsinki-NLP/opus-100). `OPUS-100` es un corpus multilingüe centrado en inglés que abarca 100 idiomas. Al ser centrado en inglés, todos los pares de entrenamiento incluyen inglés ya sea en el lado de origen o en el de destino. Los idiomas se seleccionaron según la cantidad de datos paralelos disponibles en OPUS. El corpus contiene aproximadamente 55 millones de pares de oraciones. De los 99 pares de idiomas, 44 tienen 1 millón de pares de oraciones de datos de entrenamiento, 73 tienen al menos 100 mil y 95 tienen al menos 10 mil. La tarea principal que soporta es la traducción automática."
      ],
      "metadata": {
        "id": "P7-yHTuVYba-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msicqPSIg6YZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-es\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraer solo el texto en español\n",
        "textos_espanol = []\n",
        "\n",
        "for ejemplo in dataset['train']:\n",
        "    texto_es = ejemplo['translation']['es']\n",
        "    textos_espanol.append(texto_es)\n",
        "\n",
        "# Algunos ejemplos\n",
        "print(textos_espanol[:5])\n",
        "print(f\"\\nTotal de oraciones: {len(textos_espanol)}\")"
      ],
      "metadata": {
        "id": "2rWnpwjZg8KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Versión preprocesada"
      ],
      "metadata": {
        "id": "peRbyaSEZ3je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "ISPiNjvUaGn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1CkJEbUJOB6u83_r3jhGq_KTh4NRRices"
      ],
      "metadata": {
        "id": "T24ODdyvpcU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('opus100_subtitulos_esp.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "FX_AEcI_IYhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'matar'\n",
        "\n",
        "target_docs_idxs = []\n",
        "\n",
        "for idx in df.index.to_list():\n",
        "    doc = df.loc[idx,'texto']\n",
        "    if word in word_tokenize(doc):\n",
        "        target_docs_idxs.append(idx)"
      ],
      "metadata": {
        "id": "KbpDgdi4xBxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "word = 'matar'\n",
        "replacing_words = ['m4t4r','desvivir']\n",
        "idx_to_replace = np.random.choice(target_docs_idxs,size=300,replace=False)\n",
        "# docs = list(df['texto sin numeros'].values)\n",
        "\n",
        "for idx in idx_to_replace:\n",
        "    doc = df.loc[idx,'texto']\n",
        "    new_word = np.random.choice(replacing_words,size=1)[0]\n",
        "    doc = doc.replace(word,new_word)\n",
        "    df.loc[idx,'texto'] = doc\n",
        "    # docs.append(doc)"
      ],
      "metadata": {
        "id": "4gpop0WxxYf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_docs = list(df['texto'].values)\n",
        "training_docs = [word_tokenize(doc) for doc in training_docs]"
      ],
      "metadata": {
        "id": "xgC6O5GrM38z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(training_docs, vector_size=300, window=5, min_count=2, workers=4)"
      ],
      "metadata": {
        "id": "lAmWpvANPv9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('matar')"
      ],
      "metadata": {
        "id": "FeJZSTBLQUlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparemos la similitud de estas alteraciones léxicas con respecto a la original"
      ],
      "metadata": {
        "id": "yzMQrkNma3so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mas_similares = model.wv.most_similar('matar',topn=200)\n",
        "similaridades = [similaridad for _, similaridad in mas_similares]\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(similaridades)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "czwVziKQUGIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicaciones\n",
        "\n",
        "* Moderación de contenido en redes sociales y foros.\n",
        "* Detección de spam o phishing: identificar mensajes maliciosos que evitan palabras clave bloqueadas (ej. “gr4tis”, “c1ick aquí”).\n",
        "* Prevención de acoso en línea: detectar insultos camuflados mediante alteraciones ortográficas o uso de emojis/símbolos.\n",
        "* Identificación de jerga en contextos específicos: por ejemplo, en comunidades online donde se crean nuevas formas para evitar censura.\n",
        "* Mejora de sistemas de autocorrección o normalización de texto: mapear variantes informales o erróneas a sus formas canónicas usando vecindad semántica."
      ],
      "metadata": {
        "id": "o6006uYQgsdc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i7EojuUyViDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}