{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-MCD/blob/main/02-Preprocesamiento_b%C3%A1sico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM8kLxUEVc3Z"
      },
      "source": [
        "<h1>Preprocesamiento de Texto</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta notebook visualizaremos algunos conceptos del preprocesamiento de texto, particularmente:\n",
        "\n",
        "* Tokenizaci√≥n\n",
        "* Stemming\n",
        "* Named Entity Recognition\n",
        "* POS Tagging\n",
        "\n",
        "Para esto, estudiaremos algunos ejemplos que ser√°n procesados por las clases del m√≥dulo [spaCy](https://spacy.io/). No es una gu√≠a minuciosa del uso de este m√≥dulo, son solamente algunos ejemplos ilustrativos."
      ],
      "metadata": {
        "id": "FjjdqP_uGz8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificar que la versi√≥n de spaCy sea la 3.*"
      ],
      "metadata": {
        "id": "yoeMnFzyExkd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-FDdbc62VHd"
      },
      "source": [
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vW9svTE289D"
      },
      "source": [
        " import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfJKSJEU2U_s"
      },
      "source": [
        "Ya que hemos importado spaCy, necesitamos cargar un modelo estad√≠stico de lenguaje. SpaCy ofrece una variedad de modelos para diferentes idiomas. Estos modelos ayudan con la tokenization, etiquetado PoS (part-of-speech), NER (named entity recognition) y m√°s.\n",
        "\n",
        "Bajamos y cargamos el modelo estad√≠stico de lenguaje **en_core_web_sm**, es el modelo m√°s peque√±o en ingl√©s de spaCy y un buen punto de partida.\n",
        "\n",
        "Documentaci√≥n del modelo: https://spacy.io/models/en#en_core_web_sm<br>\n",
        "Modelos disponibles: https://spacy.io/models<br>\n",
        "Uso de los modelos: https://spacy.io/usage/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7YCbWtG3LJO"
      },
      "source": [
        "üîµ Informaci√≥n adicional del modelo\n",
        "\n",
        "**en_core_web_sm** fu√© entrenado en el corpus OntoNotes 5, el cual es un corpus anotado que contiene noticias, blogs, transcripciones, etc. Los documentos del corpus est√°n anotados con informaci√≥n de como cada oraci√≥n deber√≠a *parsearse* (parsing), part-of-speech de cada palabra, si cada palabra es una *named entity*, entre otras cosas.\n",
        "\n",
        "https://catalog.ldc.upenn.edu/LDC2013T19\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En principio no hay necesidad de bajarlo"
      ],
      "metadata": {
        "id": "rppa550z4ElB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uOyHDNb2i5d"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWDrpxDk2_r2"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvF_udvi3OTO"
      },
      "source": [
        "Ya hemos cargado el modelo, la variable `nlp` hace referencia a una instancia de la clase [`Language`](https://spacy.io/api/language) que contiene metodos para varias tareas (tokenizaci√≥n, etc.) y un pipeline de procesamiento.\n",
        "\n",
        "Usaremos este modelo de lenguaje para realizar algunas tareas de preprocesamiento de PLN.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAYGtQpT3UNN"
      },
      "source": [
        "type(nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unmnGRu8D-wa"
      },
      "source": [
        "# Tokenizaci√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13twUCp2i_p8"
      },
      "source": [
        "Al pasar cualquier texto a la instancia `nlp` obtenemos un objeto [`Doc`](https://spacy.io/api/doc) que contiene el texto tokenizado e informaci√≥n adicional para cada [token](https://spacy.io/api/token)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIoEJZ-IkHQ4"
      },
      "source": [
        "# Sample sentence.\n",
        "text = \"He didn't want to pay $20 for this book.\"\n",
        "doc = nlp(text)\n",
        "print(doc,'\\n',type(doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWZK3ZSk9-f"
      },
      "source": [
        "Veamos los tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SzqhZuulAe1"
      },
      "source": [
        "print([t.text for t in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai1obkB93GdD"
      },
      "source": [
        "Observar que:\n",
        "- \"didn't\" se separa en \"did\" y \"n't\".\n",
        "- El s√≠mbolo de moneda y el n√∫mero est√°n separados.\n",
        "- El punto final es tambi√©n un token."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una tokenizaci√≥n *naive*:"
      ],
      "metadata": {
        "id": "9JmhbtseGapP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "id": "8qVPLl7lGYLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWH49gIh3hqN"
      },
      "source": [
        "El objeto `Doc` puede ser indexado y *sliced* como si fuera una lista:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwLrxRsE3oKI"
      },
      "source": [
        "print(doc[0])\n",
        "print(type(doc[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtL2IgIAGOd9"
      },
      "source": [
        "print(doc[:3])\n",
        "print(doc[-5:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TqE980F4Vrt"
      },
      "source": [
        "Podemos recuperar el texto original:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjXb8mR_DK-1"
      },
      "source": [
        "print(doc.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lume_1UP6ySQ"
      },
      "source": [
        "Podemos tokenizar multiples oraciones y accesar a ellas individualmente usando la propiedad `sents` del objeto `Doc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPZ86x0hDK4m"
      },
      "source": [
        "s = \"\"\"Either the well was very deep, or she fell very slowly, for she\n",
        "had plenty of time as she went down to look about her and to wonder what\n",
        "was going to happen next. First, she tried to look down and make out what\n",
        "she was coming to, but it was too dark to see anything; then she looked at\n",
        "the sides of the well, and noticed that they were filled with cupboards and\n",
        "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
        "\n",
        "doc = nlp(s)\n",
        "\n",
        "lista_oraciones = [sent for sent in doc.sents]\n",
        "\n",
        "print(lista_oraciones)\n",
        "print(len(lista_oraciones))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunos casos de errores en la tokenizaci√≥n: https://github.com/explosion/spaCy/issues/3052\n",
        "\n"
      ],
      "metadata": {
        "id": "oC65vt9nMujt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocimiento de entidades nombradas (NER)\n",
        "\n",
        "Podemos buscar entidades nombradas en el texto usando las anotaciones de los tokens"
      ],
      "metadata": {
        "id": "2X0XGhAZBVVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entities = list(doc.ents)\n",
        "\n",
        "print(entities)\n",
        "print(entities[0].label_)"
      ],
      "metadata": {
        "id": "RsZAmHNerYE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp('''I have $20 worth of things to buy in New York city,\n",
        "I can afford more things in Mexico or Latin America, certainly not the Starry Night painting though.\n",
        "Lastly, I think I met Zeus at a McKenzie group consulting meeting in NYC last summer.''')\n",
        "\n",
        "[(x,x.label_) for x in list(doc2.ents)]"
      ],
      "metadata": {
        "id": "cPOHGRz3-G-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que el NER no agrupa tokens juntos, estas agrupaciones se hacen de forma separada en el atributo `ents`."
      ],
      "metadata": {
        "id": "DghNxa68hujT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[x.text for x in doc2]"
      ],
      "metadata": {
        "id": "G-A6ISAI4s72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunas etiquetas de las entidades son:\n",
        "\n",
        "* PERSON People, including fictional\n",
        "* NORP Nationalities or religious or political groups\n",
        "* FACILITY Buildings, airports, highways, bridges, etc.\n",
        "* ORGANIZATION Companies, agencies, institutions, etc.\n",
        "* GPE Countries, cities, states\n",
        "* LOCATION Non-GPE locations, mountain ranges, bodies of water\n",
        "* PRODUCT Vehicles, weapons, foods, etc. (Not services)\n",
        "* EVENT Named hurricanes, battles, wars, sports events, etc.\n",
        "* WORK OF ART Titles of books, songs, etc.\n",
        "* LAW Named documents made into laws\n",
        "* LANGUAGE Any named language"
      ],
      "metadata": {
        "id": "641Yg6FP_hhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE')"
      ],
      "metadata": {
        "id": "fdb9N8GSnCN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy cuenta con algunos visualizadores para NER y otras tareas.\n",
        "\n",
        "https://spacy.io/usage/visualizers"
      ],
      "metadata": {
        "id": "O9y_VqJSmmyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc2, style='ent', jupyter=True)"
      ],
      "metadata": {
        "id": "Rx8sqbEemr5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos obtener, adem√°s, los √≠ndices de cada entidad en el texto original."
      ],
      "metadata": {
        "id": "crPob1-HnX16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc2.ents])"
      ],
      "metadata": {
        "id": "CHcdMVf4nV2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para corpus domain-specific, un etiquetador NER puede ser afinado. En este ejemplo ser√≠a bueno que _The Martian_ fuera etiquetado como \"FILM\".\n",
        "\n",
        "üîµ Esto puede aparecer m√°s adelante en los ejercicios y en el curso."
      ],
      "metadata": {
        "id": "PkNmqelTwTLZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bcIaah29MME"
      },
      "source": [
        "s = \"Ridley Scott directed The Martian.\"\n",
        "doc = nlp(s)\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('NORP')"
      ],
      "metadata": {
        "id": "v1lPRRa_nrSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvSfDUyK06Qg"
      },
      "source": [
        "## Ejercicios\n",
        "\n",
        "‚≠ï ¬øQu√© utilidad puede tener la tokenizaci√≥n?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Exploraci√≥n descriptiva de documentos mediante tokenizaci√≥n\n",
        "\n",
        "En este primer ejemplo describiremos el contenido de varios documentos que tratan sobre baseball. Exploraremos los t√©rminos m√°s frecuentes mediante la construcci√≥n de una nube de palabras. Haremos el ejercicios usando, y sin usar, tokenizaci√≥n."
      ],
      "metadata": {
        "id": "DKfBG7XNFCBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wordcloud"
      ],
      "metadata": {
        "id": "r0RZbHolIhjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "docs_newsgroups = fetch_20newsgroups(subset='train',\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=['rec.sport.baseball']\n",
        "                                     )\n",
        "\n",
        "lista_docs_20ng = docs_newsgroups.data\n",
        "print(f\"{len(lista_docs_20ng)} documentos\")"
      ],
      "metadata": {
        "id": "m0Fg0I1AID-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin tokenizar:"
      ],
      "metadata": {
        "id": "sNJkBEFbGYZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wc = WordCloud(background_color=\"white\")\n",
        "wc_img = wc.generate(\" \".join(lista_docs_20ng))\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(wc_img, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "spZrvhkhIrkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizando:"
      ],
      "metadata": {
        "id": "h8mfvdHyGZ7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "docs_spacy = nlp(\" \".join(lista_docs_20ng))\n",
        "lista_tokens = [t.text for t in docs_spacy]\n",
        "\n",
        "wc = WordCloud(background_color=\"white\")\n",
        "wc_img = wc.generate(\" \".join(lista_tokens))\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(wc_img, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tGJSMNBOLTXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualicemos los tokens junto con su frecuencia:"
      ],
      "metadata": {
        "id": "cl7ZhTO2HalP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "dict(sorted(Counter(lista_tokens).items(), key=lambda x: x[1], reverse=True))"
      ],
      "metadata": {
        "id": "nHVMTCUGL5ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora quitemos stopwords"
      ],
      "metadata": {
        "id": "ELUBSTQJ52AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_tokens = [t.text for t in docs_spacy\n",
        "                if t.text not in [\"n't\",\".\",\" \",\"'s\",\"_\",\"\\n\"]]\n",
        "\n",
        "wc = WordCloud(background_color=\"white\")\n",
        "wc_img = wc.generate(\" \".join(lista_tokens))\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(wc_img, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oHFv0yRgGJnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imprime una lista de todas las entidades nombradas que aparecen en estos documentos\n",
        "\n",
        "Al ser de baseball podr√≠an aparecer nombres de jugadores y equipos, ¬øel NER los atrapa?"
      ],
      "metadata": {
        "id": "Ds4Ss5nMiIvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXJe-9KciZWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tokenizando con NLTK y NLP-Core"
      ],
      "metadata": {
        "id": "AkzNOzzsLnVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NLTK](https://www.nltk.org/) (**N**atural **L**anguage **T**ool**K**it) es otro m√≥dulo para tareas de NLP.  Usando los tokenizadores `TreebankWordTokenizer` y `word_tokenize` de NLTK, tokeniza el parrafo un texto de tu elecci√≥n y compara la tokenizaci√≥n con spacy. ¬øEs la misma?\n",
        "\n",
        "[Core-NLP](https://stanfordnlp.github.io/CoreNLP/) es una suite para tareas de NLP, est√° escrita en Java, por lo que no se puede usar directamente en Python. Puedes usar el [demo online](https://corenlp.run/). Tokeniza el mismo texto usando esta herramienta. ¬øQu√© m√°s informaci√≥n puedes ver?\n"
      ],
      "metadata": {
        "id": "fFv1-HgQsU6a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMArLP91DKUW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Tokenizando en espa√±ol"
      ],
      "metadata": {
        "id": "Xwu6JnYBLdQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Prueba un modelo en espa√±ol de scipy para tokenizar el siguiente texto. Inspecciona la lista de tokens. ¬øNotas alg√∫n caso particular de inter√©s?\n",
        "* Revisa las entidades nombradas que reconoce, ¬øle falt√≥ alguna? ¬øhay alguna que no sea correcta?"
      ],
      "metadata": {
        "id": "ZxMaJsBY59gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = '''\n",
        "Edward Benjamin Britten (Lowestoft, 22 de noviembre de 1913-Aldeburgh, 4 de diciembre de 1976), fue un compositor, director de orquesta y pianista brit√°nico. Fue una figura central de la m√∫sica brit√°nica del siglo XX, con un abanico de obras que incluye √≥pera, otra m√∫sica vocal y piezas orquestales y de c√°mara. Entre sus obras m√°s conocidas figuran la √≥pera Peter Grimes (1945), el R√©quiem de guerra (1962) y la pieza orquestal The Young Person's Guide to the Orchestra (1945).\n",
        "'''"
      ],
      "metadata": {
        "id": "GfQjlKKN55bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Usando los tokenizadores mencionados arriba, ¬øla tokenizaci√≥n separa prefijos? Prueba en espa√±ol e ingl√©s."
      ],
      "metadata": {
        "id": "e4QS2cWve8RW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rauwBSYxfG7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Revisa los contenidos del curso [spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101)\n",
        "\n",
        "Te ayudar√° a identificar qu√© tipo de cosas se pueden hacer con spacy para futuras referencias."
      ],
      "metadata": {
        "id": "-xcpoBisJzka"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUsfYCpVT4nI"
      },
      "source": [
        "# M√°s preprocesamiento: May√∫sculas/Min√∫sculas, Stop Words Removal, Stemming y Lematizaci√≥n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgDDrCeI8f-4"
      },
      "source": [
        "spaCy performs all these preprocessing steps (except stemming) behind the scenes for you. Inline with its non-destructive policy, the tokens aren't modified directly. Rather, each **Token** object has a number of attributes which can help you get views of your document with these pre-processing steps applied. The attributes a **Token** has can be found here:<br>\n",
        "https://spacy.io/api/token#attributes\n",
        "<br><br>\n",
        "More information about spaCy's processing pipeline:<br>\n",
        "https://spacy.io/usage/processing-pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDEMR6En1j3H"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "s = \"Scientists have developed a new, more energy-efficient way for AI algorithms to process data. His model may become the basis for a new generation of AI that learns like we do.\"\n",
        "doc = nlp(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwA1ct0obYlR"
      },
      "source": [
        "## May√∫sculas/Min√∫sculas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBPWrVd9BrK"
      },
      "source": [
        "Podemos escribir los tokens en min√∫sculas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nt4RpzdgQQL"
      },
      "source": [
        "print([t.lower_ for t in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL46I4sH9OMq"
      },
      "source": [
        "Esto nos da flexibilidad para realizar otras tareas, como no cambiar a min√∫scula si es el inicio de una oraci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO0PQ8IFhOlZ"
      },
      "source": [
        "print([t.lower_ if not t.is_sent_start else t for t in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7pTz8XJbmaT"
      },
      "source": [
        "## Stop Word Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZLqqmHa9cRx"
      },
      "source": [
        "Las stop words son palabras de una lista (stoplist) que se filtran antes o despu√©s del procesamiento de texto ya que se consideran insignificantes.\n",
        "\n",
        "spaCy incluye una lista por default de stop words. Observa que cada token viene anotado con el atributo `is_stop`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAS1xmgOhO5y"
      },
      "source": [
        "print([t for t in doc if not t.is_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pdemos ver todas las stopwords. Podemos recuperarlas desde el m√≥dulo de spacy, o desde el modelo de lenguaje."
      ],
      "metadata": {
        "id": "ZHSJvEEr2d2Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kvXbuDEhOxu"
      },
      "source": [
        "from spacy.lang.en import stop_words\n",
        "\n",
        "# --- Manera 1\n",
        "stop_words = stop_words.STOP_WORDS\n",
        "print(stop_words)\n",
        "\n",
        "# --- Manera 2\n",
        "stop_words_model = nlp.Defaults.stop_words\n",
        "print(stop_words_model)\n",
        "print(len(nlp.Defaults.stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploremos los signos de puntuaci√≥n:"
      ],
      "metadata": {
        "id": "tNaOQ6PTb6-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "punctuations = list(punctuation)\n",
        "print(punctuations)\n",
        "print(len(punctuations))"
      ],
      "metadata": {
        "id": "VfPv6dWV2Opq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adem√°s de quitar stop words tambi√©n podemos quitar los signos de puntuaci√≥n:"
      ],
      "metadata": {
        "id": "4bBcoOhVazaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([t for t in doc if not t.is_stop and t.text not in punctuations])"
      ],
      "metadata": {
        "id": "fNFb5FEbam0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rETY3meeawxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPd1aiLrbqcK"
      },
      "source": [
        "## Lematizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKidP32Y_qcE"
      },
      "source": [
        "La **lematizaci√≥n** es el proceso de reducir una palabra a su forma base (lema). Se utiliza para:\n",
        "\n",
        "* Reducir la dimensionalidad del espacio de caracter√≠sticas, al mapear palabras relacionadas a un solo lema.\n",
        "* Mejorar la precisi√≥n de los modelos de lenguaje, al tratar palabras con el mismo significado como una sola entidad.\n",
        "* Facilitar la comparaci√≥n y el an√°lisis de textos, al estandarizar la forma de las palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhdRleESkzTu"
      },
      "source": [
        "[(t.text, t.lemma_) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los que cambiaron:"
      ],
      "metadata": {
        "id": "NiWEZEoveOOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(t.text, t.lemma_) for t in doc if t.text != t.lemma_]"
      ],
      "metadata": {
        "id": "TvIjPISAcPFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "text = \"He was running late, we were waiting, now we are here. Being together is the best\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(text)\n",
        "[(t.text, t.lemma_) for t in doc]"
      ],
      "metadata": {
        "id": "QdPFbBSFGK5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuaQJPjEjADE"
      },
      "source": [
        "## Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repite la nube de palabras con los documentos del 20newsgroups\n",
        "\n",
        "Produciremos nubes de palabras secuenciales:\n",
        "\n",
        "* Quitando stopwords y signos de puntuaci√≥n.\n",
        "* Quitando stopwords y puntuaci√≥n, adem√°s aplicando lematizaci√≥n.\n",
        "* Quitando stopwords y puntuaci√≥n, adem√°s aplicando stemming (spacy no lo soporta nativamente, busca opciones en NLTK).\n",
        "\n",
        "¬øCu√°l consideras que captura de manera m√°s clara la tem√°tica de los documentos?"
      ],
      "metadata": {
        "id": "8izHb8YkjU0W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDVK1gpGjUAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repite la estrategia que hayas escogido como la mejor para analizar ahora otro conjunto de documentos.\n",
        "\n",
        "Escoge documentos de otro tema del mismo corpus 20newsgroups. Aplica la misma estrategia anterior para producir dos nubes de palabras.\n"
      ],
      "metadata": {
        "id": "aJIO627Cj_hd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFxgBmbvlK-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬øConsideras que ambas nubes de palabras reflejan las tem√°ticas subyacentes de los documentos?"
      ],
      "metadata": {
        "id": "IiIOmVRalLyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gh63SGmhlLdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9HLYYUt1kOP"
      },
      "source": [
        "## Etiquetado Part-of-Speech\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr5SqjHwSWpI"
      },
      "source": [
        "spaCy realiza, como parte de su pipeline, el etiquetado Part-of-Speech (POS)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shgWRMCq1kmy"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "s = \"John watched an old movie at the cinema.\"\n",
        "doc = nlp(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9LDzULTW1_"
      },
      "source": [
        "Las etiquetas POS se pueden ver en el atributo `pos_`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-9YRcSZ1kqq"
      },
      "source": [
        "[(t.text, t.pos_) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con `spacy.explain` podemos obtener una descripci√≥n de las abreviaturas."
      ],
      "metadata": {
        "id": "6UZcgwejnYm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('PROPN')"
      ],
      "metadata": {
        "id": "D9SXNvnmnW5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_WbFDZ-Tqu9"
      },
      "source": [
        "Con el atributo `tag_` podemos obtener informaci√≥n m√°s detallada, acerca de cada token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z5oDzNr1kt2"
      },
      "source": [
        "[(t.text, t.tag_) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPOaN9yOUN-I"
      },
      "source": [
        "Descripci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnfLDxoG1kxf"
      },
      "source": [
        "print(spacy.explain('NNP'))\n",
        "print(spacy.explain('VBD'))\n",
        "print(spacy.explain('JJ'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noGuG3JvcEfs"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppWrztdJeO3J"
      },
      "source": [
        "SpaCy tambi√©n realiza el parsing como parte de su pipeline. Visualicemos un ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrvfA1TEvjJT"
      },
      "source": [
        "s = \"She enrolled in the course at the university.\"\n",
        "doc = nlp(s)\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRN7_SQ-fO5H"
      },
      "source": [
        "Para cada par de dependencias, spaCy muestra el nodo *hijo* (hacia donde apunta la flecha), el nodo *padre* (desde donde se apunta) y su relaci√≥n (la etiqueta sobre la flecha).\n",
        "\n",
        "M√°s informaci√≥n: https://spacy.io/api/annotation#dependency-parsing\n",
        "\n",
        "Como siempre, podemos ver m√°s detalles con `spacy.explain`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvz1bLTZfqmv"
      },
      "source": [
        "print(spacy.explain('nsubj'))\n",
        "print(spacy.explain('dobj'))\n",
        "print(spacy.explain('prep'))\n",
        "print(spacy.explain('pobj'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCvHyqHggIpd"
      },
      "source": [
        "Podemos acceder a las etiquetas de dependenciaa trav√©s del atributo `dep_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX_BgpMVoNaj"
      },
      "source": [
        "[(t.text, t.dep_) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt7zLq0ugR7O"
      },
      "source": [
        "Observa que 'enrolled' es el _ROOT_. A continuaci√≥n podemos darnos una idea de c√≥mo ocurre el etiquetado en el pipeline.\n",
        "\n",
        "* nsubj (nominal subject) = SUJETO de la oraci√≥n\n",
        "* ROOT = El verbo principal de la oraci√≥n\n",
        "* obj (direct object) = objeto directo del verbo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X15EOIq0oNfF"
      },
      "source": [
        "[(t.text, t.dep_, t.pos_, t.head.text) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As√≠ podemos acceder a los nodos descendientes de un nodo en particular:"
      ],
      "metadata": {
        "id": "HJ_pHAam6y0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in doc:\n",
        "    if \"obj\" in t.dep_:\n",
        "        print(' '.join([child.text for child in t.subtree]))"
      ],
      "metadata": {
        "id": "CSBHfdXf6HwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analicemos los descendientes de un nodo `ROOT`"
      ],
      "metadata": {
        "id": "0gRSQ15U78TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in doc:\n",
        "    if \"ROOT\" in t.dep_:\n",
        "        # print(' '.join([child.text for child in t.subtree if child.dep_ != 'ROOT']))\n",
        "        print(' '.join([child.text for child in t.subtree if child.pos_ != 'VERB']))"
      ],
      "metadata": {
        "id": "RZkH6ANT6oyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos acceder y reconstruir la estructura gramatical de una oraci√≥n de un documento:"
      ],
      "metadata": {
        "id": "4OOOXWc59EXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in doc.sents:\n",
        "    for token in sent:\n",
        "        if token.dep_ == 'nsubj':\n",
        "            subject = ' '.join([child.text for child in token.subtree])\n",
        "            print(f\"Sujeto: {subject}\")\n",
        "        elif token.dep_ == 'ROOT' and token.pos_ == 'VERB':\n",
        "            print(f\"Verbo: {token.text}\")\n",
        "            predicado = ' '.join([child.text for child in token.subtree if child.dep_!= 'nsubj' and child.dep_ != 'ROOT'])\n",
        "            print(f\"Objeto: {predicado}\")"
      ],
      "metadata": {
        "id": "DYYT6u3U7RFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uEG3Va6p8HIr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}