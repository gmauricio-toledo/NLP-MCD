{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-MCD/blob/main/02-Preprocesamiento_b%C3%A1sico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM8kLxUEVc3Z"
      },
      "source": [
        "<h1>Preprocesamiento de Texto</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta notebook visualizaremos algunos conceptos del preprocesamiento de texto, particularmente:\n",
        "\n",
        "* Tokenización\n",
        "* Stemming\n",
        "* Named Entity Recognition\n",
        "* POS Tagging\n",
        "\n",
        "Para esto, estudiaremos algunos ejemplos que serán procesados por las clases del módulo [spaCy](https://spacy.io/). No es una guía minuciosa del uso de este módulo, son solamente algunos ejemplos ilustrativos."
      ],
      "metadata": {
        "id": "FjjdqP_uGz8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificar que la versión de spaCy sea la 3.*"
      ],
      "metadata": {
        "id": "yoeMnFzyExkd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-FDdbc62VHd"
      },
      "source": [
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vW9svTE289D"
      },
      "source": [
        " import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfJKSJEU2U_s"
      },
      "source": [
        "Ya que hemos importado spaCy, necesitamos cargar un modelo estadístico de lenguaje. SpaCy ofrece una variedad de modelos para diferentes idiomas. Estos modelos ayudan con la tokenization, etiquetado PoS (part-of-speech), NER (named entity recognition) y más.\n",
        "\n",
        "Bajamos y cargamos el modelo estadístico de lenguaje **en_core_web_sm**, es el modelo más pequeño en inglés de spaCy y un buen punto de partida.\n",
        "\n",
        "Documentación del modelo: https://spacy.io/models/en#en_core_web_sm<br>\n",
        "Modelos disponibles: https://spacy.io/models<br>\n",
        "Uso de los modelos: https://spacy.io/usage/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7YCbWtG3LJO"
      },
      "source": [
        "🔵 Información adicional del modelo\n",
        "\n",
        "**en_core_web_sm** fué entrenado en el corpus OntoNotes 5, el cual es un corpus anotado que contiene noticias, blogs, transcripciones, etc. Los documentos del corpus están anotados con información de como cada oración debería *parsearse* (parsing), part-of-speech de cada palabra, si cada palabra es una *named entity*, entre otras cosas.\n",
        "\n",
        "https://catalog.ldc.upenn.edu/LDC2013T19\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En principio no hay necesidad de bajarlo"
      ],
      "metadata": {
        "id": "rppa550z4ElB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uOyHDNb2i5d"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWDrpxDk2_r2"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvF_udvi3OTO"
      },
      "source": [
        "Ya hemos cargado el modelo, la variable `nlp` hace referencia a una instancia de la clase [`Language`](https://spacy.io/api/language) que contiene metodos para varias tareas (tokenización, etc.) y un pipeline de procesamiento.\n",
        "\n",
        "Usaremos este modelo de lenguaje para realizar algunas tareas de preprocesamiento de PLN.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAYGtQpT3UNN"
      },
      "source": [
        "type(nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unmnGRu8D-wa"
      },
      "source": [
        "# Tokenización\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13twUCp2i_p8"
      },
      "source": [
        "Al pasar cualquier texto a la instancia `nlp` obtenemos un objeto [`Doc`](https://spacy.io/api/doc) que contiene el texto tokenizado e información adicional para cada [token](https://spacy.io/api/token)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIoEJZ-IkHQ4"
      },
      "source": [
        "# Sample sentence.\n",
        "text = \"He didn't want to pay $20 for this book.\"\n",
        "doc = nlp(text)\n",
        "print(doc,'\\n',type(doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWZK3ZSk9-f"
      },
      "source": [
        "Veamos los tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SzqhZuulAe1"
      },
      "source": [
        "print([t.text for t in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai1obkB93GdD"
      },
      "source": [
        "Observar que:\n",
        "- \"didn't\" se separa en \"did\" y \"n't\".\n",
        "- El símbolo de moneda y el número están separados.\n",
        "- El punto final es también un token."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una tokenización *naive*:"
      ],
      "metadata": {
        "id": "9JmhbtseGapP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "id": "8qVPLl7lGYLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWH49gIh3hqN"
      },
      "source": [
        "El objeto `Doc` puede ser indexado y *sliced* como si fuera una lista:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwLrxRsE3oKI"
      },
      "source": [
        "print(doc[0])\n",
        "print(type(doc[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtL2IgIAGOd9"
      },
      "source": [
        "print(doc[:3])\n",
        "print(doc[-5:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TqE980F4Vrt"
      },
      "source": [
        "Podemos recuperar el texto original:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjXb8mR_DK-1"
      },
      "source": [
        "print(doc.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lume_1UP6ySQ"
      },
      "source": [
        "Podemos tokenizar multiples oraciones y accesar a ellas individualmente usando la propiedad `sents` del objeto `Doc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPZ86x0hDK4m"
      },
      "source": [
        "s = \"\"\"Either the well was very deep, or she fell very slowly, for she\n",
        "had plenty of time as she went down to look about her and to wonder what\n",
        "was going to happen next. First, she tried to look down and make out what\n",
        "she was coming to, but it was too dark to see anything; then she looked at\n",
        "the sides of the well, and noticed that they were filled with cupboards and\n",
        "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
        "\n",
        "doc = nlp(s)\n",
        "\n",
        "lista_oraciones = [sent for sent in doc.sents]\n",
        "\n",
        "print(lista_oraciones)\n",
        "print(len(lista_oraciones))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunos casos de errores en la tokenización: https://github.com/explosion/spaCy/issues/3052\n",
        "\n"
      ],
      "metadata": {
        "id": "oC65vt9nMujt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocimiento de entidades nombradas (NER)\n",
        "\n",
        "Podemos buscar entidades nombradas en el texto usando las anotaciones de los tokens"
      ],
      "metadata": {
        "id": "2X0XGhAZBVVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entities = list(doc.ents)\n",
        "\n",
        "print(entities)\n",
        "print(entities[0].label_)"
      ],
      "metadata": {
        "id": "RsZAmHNerYE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp('''I have $20 worth of things to buy in New York city,\n",
        "I can afford more things in Mexico or Latin America, certainly not the Starry Night painting though.\n",
        "Lastly, I think I met Zeus at a McKenzie group consulting meeting in NYC last summer.''')\n",
        "\n",
        "[(x,x.label_) for x in list(doc2.ents)]"
      ],
      "metadata": {
        "id": "cPOHGRz3-G-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que el NER no agrupa tokens juntos, estas agrupaciones se hacen de forma separada en el atributo `ents`."
      ],
      "metadata": {
        "id": "DghNxa68hujT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[x.text for x in doc2]"
      ],
      "metadata": {
        "id": "G-A6ISAI4s72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunas etiquetas de las entidades son:\n",
        "\n",
        "* PERSON People, including fictional\n",
        "* NORP Nationalities or religious or political groups\n",
        "* FACILITY Buildings, airports, highways, bridges, etc.\n",
        "* ORGANIZATION Companies, agencies, institutions, etc.\n",
        "* GPE Countries, cities, states\n",
        "* LOCATION Non-GPE locations, mountain ranges, bodies of water\n",
        "* PRODUCT Vehicles, weapons, foods, etc. (Not services)\n",
        "* EVENT Named hurricanes, battles, wars, sports events, etc.\n",
        "* WORK OF ART Titles of books, songs, etc.\n",
        "* LAW Named documents made into laws\n",
        "* LANGUAGE Any named language"
      ],
      "metadata": {
        "id": "641Yg6FP_hhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE')"
      ],
      "metadata": {
        "id": "fdb9N8GSnCN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy cuenta con algunos visualizadores para NER y otras tareas.\n",
        "\n",
        "https://spacy.io/usage/visualizers"
      ],
      "metadata": {
        "id": "O9y_VqJSmmyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc2, style='ent', jupyter=True)"
      ],
      "metadata": {
        "id": "Rx8sqbEemr5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos obtener, además, los índices de cada entidad en el texto original."
      ],
      "metadata": {
        "id": "crPob1-HnX16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc2.ents])"
      ],
      "metadata": {
        "id": "CHcdMVf4nV2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para corpus domain-specific, un etiquetador NER puede ser afinado. En este ejemplo sería bueno que _The Martian_ fuera etiquetado como \"FILM\".\n",
        "\n",
        "🔵 Esto puede aparecer más adelante en los ejercicios y en el curso."
      ],
      "metadata": {
        "id": "PkNmqelTwTLZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bcIaah29MME"
      },
      "source": [
        "s = \"Ridley Scott directed The Martian.\"\n",
        "doc = nlp(s)\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('NORP')"
      ],
      "metadata": {
        "id": "v1lPRRa_nrSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvSfDUyK06Qg"
      },
      "source": [
        "## Ejercicios\n",
        "\n",
        "⭕ ¿Qué utilidad puede tener la tokenización?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Exploración descriptiva de documentos mediante tokenización\n",
        "\n",
        "En este primer ejemplo describiremos el contenido de varios documentos que tratan sobre baseball. Exploraremos los términos más frecuentes mediante la construcción de una nube de palabras. Haremos el ejercicios usando, y sin usar, tokenización."
      ],
      "metadata": {
        "id": "DKfBG7XNFCBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wordcloud"
      ],
      "metadata": {
        "id": "r0RZbHolIhjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "docs_newsgroups = fetch_20newsgroups(subset='train',\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=['rec.sport.baseball']\n",
        "                                     )\n",
        "\n",
        "lista_docs_20ng = docs_newsgroups.data\n",
        "print(f\"{len(lista_docs_20ng)} documentos\")"
      ],
      "metadata": {
        "id": "m0Fg0I1AID-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin tokenizar:"
      ],
      "metadata": {
        "id": "sNJkBEFbGYZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wc = WordCloud(background_color=\"white\")\n",
        "wc_img = wc.generate(\" \".join(lista_docs_20ng))\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(wc_img, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "spZrvhkhIrkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizando:"
      ],
      "metadata": {
        "id": "h8mfvdHyGZ7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "docs_spacy = nlp(\" \".join(lista_docs_20ng))\n",
        "lista_tokens = [t.text for t in docs_spacy]\n",
        "\n",
        "wc = WordCloud(background_color=\"white\")\n",
        "wc_img = wc.generate(\" \".join(lista_tokens))\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(wc_img, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tGJSMNBOLTXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualicemos los tokens junto con su frecuencia:"
      ],
      "metadata": {
        "id": "cl7ZhTO2HalP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "dict(sorted(Counter(lista_tokens).items(), key=lambda x: x[1], reverse=True))"
      ],
      "metadata": {
        "id": "nHVMTCUGL5ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora quitemos stopwords"
      ],
      "metadata": {
        "id": "ELUBSTQJ52AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_tokens = [t.text for t in docs_spacy\n",
        "                if t.text not in [\"n't\",\".\",\" \",\"'s\",\"_\",\"\\n\"]]\n",
        "\n",
        "wc = WordCloud(background_color=\"white\")\n",
        "wc_img = wc.generate(\" \".join(lista_tokens))\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(wc_img, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oHFv0yRgGJnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imprime una lista de todas las entidades nombradas que aparecen en estos documentos\n",
        "\n",
        "Al ser de baseball podrían aparecer nombres de jugadores y equipos, ¿el NER los atrapa?"
      ],
      "metadata": {
        "id": "Ds4Ss5nMiIvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXJe-9KciZWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tokenizando con NLTK y NLP-Core"
      ],
      "metadata": {
        "id": "AkzNOzzsLnVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NLTK](https://www.nltk.org/) (**N**atural **L**anguage **T**ool**K**it) es otro módulo para tareas de NLP.  Usando los tokenizadores `TreebankWordTokenizer` y `word_tokenize` de NLTK, tokeniza el parrafo un texto de tu elección y compara la tokenización con spacy. ¿Es la misma?\n",
        "\n",
        "[Core-NLP](https://stanfordnlp.github.io/CoreNLP/) es una suite para tareas de NLP, está escrita en Java, por lo que no se puede usar directamente en Python. Puedes usar el [demo online](https://corenlp.run/). Tokeniza el mismo texto usando esta herramienta. ¿Qué más información puedes ver?\n"
      ],
      "metadata": {
        "id": "fFv1-HgQsU6a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMArLP91DKUW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Tokenizando en español"
      ],
      "metadata": {
        "id": "Xwu6JnYBLdQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Prueba un modelo en español de scipy para tokenizar el siguiente texto. Inspecciona la lista de tokens. ¿Notas algún caso particular de interés?\n",
        "* Revisa las entidades nombradas que reconoce, ¿le faltó alguna? ¿hay alguna que no sea correcta?"
      ],
      "metadata": {
        "id": "ZxMaJsBY59gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = '''\n",
        "Edward Benjamin Britten (Lowestoft, 22 de noviembre de 1913-Aldeburgh, 4 de diciembre de 1976), fue un compositor, director de orquesta y pianista británico. Fue una figura central de la música británica del siglo XX, con un abanico de obras que incluye ópera, otra música vocal y piezas orquestales y de cámara. Entre sus obras más conocidas figuran la ópera Peter Grimes (1945), el Réquiem de guerra (1962) y la pieza orquestal The Young Person's Guide to the Orchestra (1945).\n",
        "'''"
      ],
      "metadata": {
        "id": "GfQjlKKN55bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Usando los tokenizadores mencionados arriba, ¿la tokenización separa prefijos? Prueba en español e inglés."
      ],
      "metadata": {
        "id": "e4QS2cWve8RW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rauwBSYxfG7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Revisa los contenidos del curso [spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101)\n",
        "\n",
        "Te ayudará a identificar qué tipo de cosas se pueden hacer con spacy para futuras referencias."
      ],
      "metadata": {
        "id": "-xcpoBisJzka"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUsfYCpVT4nI"
      },
      "source": [
        "# Más preprocesamiento: Mayúsculas/Minúsculas, Stop Words Removal, Stemming y Lematización.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgDDrCeI8f-4"
      },
      "source": [
        "spaCy performs all these preprocessing steps (except stemming) behind the scenes for you. Inline with its non-destructive policy, the tokens aren't modified directly. Rather, each **Token** object has a number of attributes which can help you get views of your document with these pre-processing steps applied. The attributes a **Token** has can be found here:<br>\n",
        "https://spacy.io/api/token#attributes\n",
        "<br><br>\n",
        "More information about spaCy's processing pipeline:<br>\n",
        "https://spacy.io/usage/processing-pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDEMR6En1j3H"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "s = \"Scientists have developed a new, more energy-efficient way for AI algorithms to process data. His model may become the basis for a new generation of AI that learns like we do.\"\n",
        "doc = nlp(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwA1ct0obYlR"
      },
      "source": [
        "## Mayúsculas/Minúsculas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBPWrVd9BrK"
      },
      "source": [
        "Podemos escribir los tokens en minúsculas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nt4RpzdgQQL"
      },
      "source": [
        "print([t.lower_ for t in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL46I4sH9OMq"
      },
      "source": [
        "Esto nos da flexibilidad para realizar otras tareas, como no cambiar a minúscula si es el inicio de una oración."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO0PQ8IFhOlZ"
      },
      "source": [
        "print([t.lower_ if not t.is_sent_start else t for t in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7pTz8XJbmaT"
      },
      "source": [
        "## Stop Word Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZLqqmHa9cRx"
      },
      "source": [
        "Las stop words son palabras de una lista (stoplist) que se filtran antes o después del procesamiento de texto ya que se consideran insignificantes.\n",
        "\n",
        "spaCy incluye una lista por default de stop words. Observa que cada token viene anotado con el atributo `is_stop`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAS1xmgOhO5y"
      },
      "source": [
        "print([t for t in doc if not t.is_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pdemos ver todas las stopwords. Podemos recuperarlas desde el módulo de spacy, o desde el modelo de lenguaje."
      ],
      "metadata": {
        "id": "ZHSJvEEr2d2Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kvXbuDEhOxu"
      },
      "source": [
        "from spacy.lang.en import stop_words\n",
        "\n",
        "# --- Manera 1\n",
        "stop_words = stop_words.STOP_WORDS\n",
        "print(stop_words)\n",
        "\n",
        "# --- Manera 2\n",
        "stop_words_model = nlp.Defaults.stop_words\n",
        "print(stop_words_model)\n",
        "print(len(nlp.Defaults.stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploremos los signos de puntuación:"
      ],
      "metadata": {
        "id": "tNaOQ6PTb6-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "punctuations = list(punctuation)\n",
        "print(punctuations)\n",
        "print(len(punctuations))"
      ],
      "metadata": {
        "id": "VfPv6dWV2Opq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además de quitar stop words también podemos quitar los signos de puntuación:"
      ],
      "metadata": {
        "id": "4bBcoOhVazaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([t for t in doc if not t.is_stop and t.text not in punctuations])"
      ],
      "metadata": {
        "id": "fNFb5FEbam0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rETY3meeawxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPd1aiLrbqcK"
      },
      "source": [
        "## Lematización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKidP32Y_qcE"
      },
      "source": [
        "La **lematización** es el proceso de reducir una palabra a su forma base (lema). Se utiliza para:\n",
        "\n",
        "* Reducir la dimensionalidad del espacio de características, al mapear palabras relacionadas a un solo lema.\n",
        "* Mejorar la precisión de los modelos de lenguaje, al tratar palabras con el mismo significado como una sola entidad.\n",
        "* Facilitar la comparación y el análisis de textos, al estandarizar la forma de las palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhdRleESkzTu"
      },
      "source": [
        "[(t.text, t.lemma_) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los que cambiaron:"
      ],
      "metadata": {
        "id": "NiWEZEoveOOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(t.text, t.lemma_) for t in doc if t.text != t.lemma_]"
      ],
      "metadata": {
        "id": "TvIjPISAcPFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "text = \"He was running late, we were waiting, now we are here. Being together is the best\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(text)\n",
        "[(t.text, t.lemma_) for t in doc]"
      ],
      "metadata": {
        "id": "QdPFbBSFGK5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuaQJPjEjADE"
      },
      "source": [
        "## Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repite la nube de palabras con los documentos del 20newsgroups\n",
        "\n",
        "Produciremos nubes de palabras secuenciales:\n",
        "\n",
        "* Quitando stopwords y signos de puntuación.\n",
        "* Quitando stopwords y puntuación, además aplicando lematización.\n",
        "* Quitando stopwords y puntuación, además aplicando stemming (spacy no lo soporta nativamente, busca opciones en NLTK).\n",
        "\n",
        "¿Cuál consideras que captura de manera más clara la temática de los documentos?"
      ],
      "metadata": {
        "id": "8izHb8YkjU0W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDVK1gpGjUAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repite la estrategia que hayas escogido como la mejor para analizar ahora otro conjunto de documentos.\n",
        "\n",
        "Escoge documentos de otro tema del mismo corpus 20newsgroups. Aplica la misma estrategia anterior para producir dos nubes de palabras.\n"
      ],
      "metadata": {
        "id": "aJIO627Cj_hd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFxgBmbvlK-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Consideras que ambas nubes de palabras reflejan las temáticas subyacentes de los documentos?"
      ],
      "metadata": {
        "id": "IiIOmVRalLyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gh63SGmhlLdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9HLYYUt1kOP"
      },
      "source": [
        "## Etiquetado Part-of-Speech\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr5SqjHwSWpI"
      },
      "source": [
        "spaCy realiza, como parte de su pipeline, el etiquetado Part-of-Speech (POS)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shgWRMCq1kmy"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "s = \"John watched an old movie at the cinema.\"\n",
        "doc = nlp(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9LDzULTW1_"
      },
      "source": [
        "Las etiquetas POS se pueden ver en el atributo `pos_`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-9YRcSZ1kqq"
      },
      "source": [
        "[(t.text, t.pos_) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con `spacy.explain` podemos obtener una descripción de las abreviaturas."
      ],
      "metadata": {
        "id": "6UZcgwejnYm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('PROPN')"
      ],
      "metadata": {
        "id": "D9SXNvnmnW5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_WbFDZ-Tqu9"
      },
      "source": [
        "Con el atributo `tag_` podemos obtener información más detallada, acerca de cada token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z5oDzNr1kt2"
      },
      "source": [
        "[(t.text, t.tag_) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPOaN9yOUN-I"
      },
      "source": [
        "Descripción:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnfLDxoG1kxf"
      },
      "source": [
        "print(spacy.explain('NNP'))\n",
        "print(spacy.explain('VBD'))\n",
        "print(spacy.explain('JJ'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noGuG3JvcEfs"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppWrztdJeO3J"
      },
      "source": [
        "SpaCy también realiza el parsing como parte de su pipeline. Visualicemos un ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrvfA1TEvjJT"
      },
      "source": [
        "s = \"She enrolled in the course at the university.\"\n",
        "doc = nlp(s)\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRN7_SQ-fO5H"
      },
      "source": [
        "Para cada par de dependencias, spaCy muestra el nodo *hijo* (hacia donde apunta la flecha), el nodo *padre* (desde donde se apunta) y su relación (la etiqueta sobre la flecha).\n",
        "\n",
        "Más información: https://spacy.io/api/annotation#dependency-parsing\n",
        "\n",
        "Como siempre, podemos ver más detalles con `spacy.explain`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvz1bLTZfqmv"
      },
      "source": [
        "print(spacy.explain('nsubj'))\n",
        "print(spacy.explain('dobj'))\n",
        "print(spacy.explain('prep'))\n",
        "print(spacy.explain('pobj'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCvHyqHggIpd"
      },
      "source": [
        "Podemos acceder a las etiquetas de dependenciaa través del atributo `dep_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX_BgpMVoNaj"
      },
      "source": [
        "[(t.text, t.dep_) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt7zLq0ugR7O"
      },
      "source": [
        "Observa que 'enrolled' es el _ROOT_. A continuación podemos darnos una idea de cómo ocurre el etiquetado en el pipeline.\n",
        "\n",
        "* nsubj (nominal subject) = SUJETO de la oración\n",
        "* ROOT = El verbo principal de la oración\n",
        "* obj (direct object) = objeto directo del verbo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X15EOIq0oNfF"
      },
      "source": [
        "[(t.text, t.dep_, t.pos_, t.head.text) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Así podemos acceder a los nodos descendientes de un nodo en particular:"
      ],
      "metadata": {
        "id": "HJ_pHAam6y0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in doc:\n",
        "    if \"obj\" in t.dep_:\n",
        "        print(' '.join([child.text for child in t.subtree]))"
      ],
      "metadata": {
        "id": "CSBHfdXf6HwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analicemos los descendientes de un nodo `ROOT`"
      ],
      "metadata": {
        "id": "0gRSQ15U78TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in doc:\n",
        "    if \"ROOT\" in t.dep_:\n",
        "        # print(' '.join([child.text for child in t.subtree if child.dep_ != 'ROOT']))\n",
        "        print(' '.join([child.text for child in t.subtree if child.pos_ != 'VERB']))"
      ],
      "metadata": {
        "id": "RZkH6ANT6oyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos acceder y reconstruir la estructura gramatical de una oración de un documento:"
      ],
      "metadata": {
        "id": "4OOOXWc59EXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in doc.sents:\n",
        "    for token in sent:\n",
        "        if token.dep_ == 'nsubj':\n",
        "            subject = ' '.join([child.text for child in token.subtree])\n",
        "            print(f\"Sujeto: {subject}\")\n",
        "        elif token.dep_ == 'ROOT' and token.pos_ == 'VERB':\n",
        "            print(f\"Verbo: {token.text}\")\n",
        "            predicado = ' '.join([child.text for child in token.subtree if child.dep_!= 'nsubj' and child.dep_ != 'ROOT'])\n",
        "            print(f\"Objeto: {predicado}\")"
      ],
      "metadata": {
        "id": "DYYT6u3U7RFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uEG3Va6p8HIr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}