{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-MCD/blob/main/09-WordEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Word and Document Embeddings</h1>\n",
        "\n",
        "En esta notebook exploraremos el uso de distintos embeddings para resolver.algunas tareas del NLP.\n",
        "\n",
        "Los puntos principales de esta notebook son:"
      ],
      "metadata": {
        "id": "Z0BPue_U2o5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from string import punctuation\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('spanish')"
      ],
      "metadata": {
        "id": "Wipz0tTqYfz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq umap-learn\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "TC8MaCrRdyYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizar_vector(v):\n",
        "    if np.linalg.norm(v) == 0:\n",
        "        return v\n",
        "    else:\n",
        "        return v / np.linalg.norm(v)"
      ],
      "metadata": {
        "id": "FLSfOXQSxatL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento y limpieza del texto"
      ],
      "metadata": {
        "id": "45by_EQTZMys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/gmauricio-toledo/NLP-MCD/main/data/spanish-wikipedia-dataframe.csv\"\n",
        "df = pd.read_csv(url,index_col=0)\n",
        "df.drop(columns='doc_id',inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "QWk9UEToYhA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_raw = df['Texto'].tolist()\n",
        "docs = [re.sub(r'\\d+', ' ', doc) for doc in docs_raw]\n",
        "tokenized_docs = [word_tokenize(doc) for doc in docs]\n",
        "docs = [' '.join(doc) for doc in tokenized_docs]\n",
        "docs[:3]"
      ],
      "metadata": {
        "id": "GXtV1G7zZMC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 2500\n",
        "\n",
        "cv = CountVectorizer(max_features=max_features,\n",
        "                     stop_words=stopwords)\n",
        "X_bow = cv.fit_transform(docs)\n",
        "X_bow.shape"
      ],
      "metadata": {
        "id": "l34UYe73ZXUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observación** Sobre las clases sparse de scipy"
      ],
      "metadata": {
        "id": "uFEAjmY9ertq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_bow)"
      ],
      "metadata": {
        "id": "rkJ16FBVeydv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analicemos la naturaleza sparse de estas representaciones"
      ],
      "metadata": {
        "id": "IgVPvtXGe1oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "idx = np.random.choice(max_features,size=1)\n",
        "\n",
        "word = cv.get_feature_names_out()[idx]\n",
        "word_dim = X_bow.shape[0]\n",
        "bow_vector = X_bow.toarray()[:,idx]\n",
        "print(f\"palabra: {word}\")\n",
        "zeros = np.where(bow_vector==0)[0].shape[0]\n",
        "print(f\"Número de entradas cero: {zeros}/{word_dim}={round(100*zeros/word_dim,2)}%\")"
      ],
      "metadata": {
        "id": "qQPunpLUd0hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word2vec\n",
        "\n",
        "Usaremos la implementación de gensim: https://radimrehurek.com/gensim/models/word2vec.html.\n",
        "\n",
        "Tensorflow también tiene una implementación ([tutorial](https://www.tensorflow.org/text/tutorials/word2vec)).\n",
        "\n",
        "El artículo original: https://arxiv.org/pdf/1301.3781"
      ],
      "metadata": {
        "id": "S9ZhhIW0K53t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usar un modelo pre-entrenado"
      ],
      "metadata": {
        "id": "YlkciEfegeJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Un modelo de gensim"
      ],
      "metadata": {
        "id": "hFoVls9OxrZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim tiene varios modelos de word2vec preentrenados:"
      ],
      "metadata": {
        "id": "cufs6u5Ogpqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "\n",
        "for x in gensim.downloader.info()['models'].keys():\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "tvKfYZ9EggE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descarguemos alguno de estos modelos.\n",
        "\n",
        "Tarda alrededor de 20 minutos"
      ],
      "metadata": {
        "id": "ALj_G2Rugtkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "\n",
        "# pt_w2v_model = gensim.downloader.load('glove-wiki-gigaword-50')\n",
        "pt_w2v_model = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "QJDfDqCwgtWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtengamos los vectores"
      ],
      "metadata": {
        "id": "zH7814yLwTT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = pt_w2v_model.vectors\n",
        "vectors.shape"
      ],
      "metadata": {
        "id": "Lc0QBSbGu3xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realicemos algunas tareas de similitud:"
      ],
      "metadata": {
        "id": "ewW4feK6wUvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resolvamos algunas analogías:"
      ],
      "metadata": {
        "id": "pns52EXLwcjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Un modelo *externo*\n",
        "\n",
        "Descarguemos un modelo externo y experimentemos con él"
      ],
      "metadata": {
        "id": "_fZm9iL9xuqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 0B7XkCwpI5KDYNlNUTTlSS21pQmM"
      ],
      "metadata": {
        "id": "u7aJ7sR2jWAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leamos el modelo, **tarda alrededor de 2 minutos**"
      ],
      "metadata": {
        "id": "GmJOzDEKGhKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "pretrained_model_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "pt_w2v_model = KeyedVectors.load_word2vec_format(pretrained_model_path, binary=True)"
      ],
      "metadata": {
        "id": "52rolJPDhq3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimentación"
      ],
      "metadata": {
        "id": "QSt-gt-Yp3Nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensión de los embeddings"
      ],
      "metadata": {
        "id": "9a84hcmIvT1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pt_w2v_model.vector_size"
      ],
      "metadata": {
        "id": "xmUY6lWhvTWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = pt_w2v_model.index_to_key\n",
        "print(f\"Tamaño del vocabulario: {len(vocabulary)}\")\n",
        "print(vocabulary[:20])"
      ],
      "metadata": {
        "id": "xV9Fn-H70t3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"king\"\n",
        "\n",
        "pt_w2v_model.most_similar(word,topn=15)"
      ],
      "metadata": {
        "id": "314d8tHDvf7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_w2v_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)\n",
        "# pt_w2v_model.most_similar(positive=['woman', 'actor'], negative=['man'], topn=5)"
      ],
      "metadata": {
        "id": "fkegjgX4wee3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos la similitud coseno entre palabras _similares_ y _no similares_"
      ],
      "metadata": {
        "id": "Kihqfma0F5kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras no similares\n",
        "word1 = \"dream\"\n",
        "word2 = \"technology\"\n",
        "similarity1 = pt_w2v_model.similarity(word1, word2)\n",
        "print(similarity1)\n",
        "\n",
        "# Palabras relativamente similares\n",
        "word5 = \"computer\"\n",
        "word6 = \"pencil\"\n",
        "similarity3 = pt_w2v_model.similarity(word5, word6)\n",
        "print(similarity3)\n",
        "\n",
        "# Palabras muy similares\n",
        "word3 = \"boy\"\n",
        "word4 = \"girl\"\n",
        "similarity2 = pt_w2v_model.similarity(word3, word4)\n",
        "print(similarity2)"
      ],
      "metadata": {
        "id": "bFQj-BlFvIQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploremos la geometría de estos embeddings:"
      ],
      "metadata": {
        "id": "JeI0R9TUwirS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
      ],
      "metadata": {
        "id": "lRKXv9T0xY1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'spain'\n",
        "\n",
        "word_vector = pt_w2v_model[word]\n",
        "\n",
        "dim = pt_w2v_model.vector_size\n",
        "print(f\"palabra: {word}\")\n",
        "zeros = np.where(word_vector==0)[0].shape[0]\n",
        "print(f\"Número de entradas 0: {zeros}/{dim}={round(100*zeros/dim,2)}%\")"
      ],
      "metadata": {
        "id": "q-sceWhzn3Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_w2v_model['spain']"
      ],
      "metadata": {
        "id": "DD78Z2rm07p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las direcciones entre pares de palabras, las cuales codifican la relación semántica entre ellas"
      ],
      "metadata": {
        "id": "M3I-BUcLKOoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = 'sweden'\n",
        "word2 = 'stockholm'\n",
        "word3 = 'france'\n",
        "word4 = 'paris'\n",
        "word5 = 'germany'\n",
        "word6 = 'berlin'\n",
        "\n",
        "w1 = pt_w2v_model[word1]\n",
        "w2 = pt_w2v_model[word2]\n",
        "w3 = pt_w2v_model[word3]\n",
        "w4 = pt_w2v_model[word4]\n",
        "w5 = pt_w2v_model[word5]\n",
        "w6 = pt_w2v_model[word6]\n",
        "\n",
        "print(f\"Palabras a analizar:\\n{word1}-{word2}\\n{word3}-{word4}\\n{word5}-{word6}\")\n",
        "dif_1, dif_2, dif_3 = w1 - w2, w3 - w4, w5 - w6\n",
        "print(\"Similitud entre las diferencias:\")\n",
        "print(cosine_similarity(dif_1, dif_2),cosine_similarity(dif_1, dif_3), cosine_similarity(dif_2, dif_3))\n",
        "\n",
        "# Ahora con palabras aleatorias:\n",
        "six_random_words = np.random.choice(vocabulary,size=6,replace=False)\n",
        "print(f\"\\nEl mismo análisis con 6 palabras aleatorias:\\n{six_random_words}\")\n",
        "rw1 = pt_w2v_model[six_random_words[0]]\n",
        "rw2 = pt_w2v_model[six_random_words[1]]\n",
        "rw3 = pt_w2v_model[six_random_words[2]]\n",
        "rw4 = pt_w2v_model[six_random_words[3]]\n",
        "rw5 = pt_w2v_model[six_random_words[4]]\n",
        "rw6 = pt_w2v_model[six_random_words[5]]\n",
        "\n",
        "dif_1, dif_2, dif_3 = rw1 - rw2, rw3 - rw4, rw5 - rw6\n",
        "print(\"Similitud entre las diferencias:\")\n",
        "print(cosine_similarity(dif_1, dif_2),cosine_similarity(dif_1, dif_3), cosine_similarity(dif_2, dif_3))"
      ],
      "metadata": {
        "id": "gYKy0JFJwiXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenar un modelo en el corpus\n",
        "\n",
        "Es importante considerar que el modelo depende mucho de los datos con los que se entrena. Para muchas tareas generales basta con utilizar un modelo preentrenado, pero algunas aplicaciones específicas (por ejemplo, específicas de un dominio especializado) pueden requerir entrenar un modelo en un corpus específico."
      ],
      "metadata": {
        "id": "EvzhD5dXgU2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=5, workers=4)"
      ],
      "metadata": {
        "id": "292Fx_UuK71J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el vocabulario obtenido"
      ],
      "metadata": {
        "id": "PXqvjokUrHGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = w2v_model.wv.index_to_key\n",
        "print(f\"Tamaño del vocabulario: {len(vocabulary)}\")\n",
        "print(vocabulary[:20])"
      ],
      "metadata": {
        "id": "Nle_vjJXrGr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los vectores:"
      ],
      "metadata": {
        "id": "Sk7XV_2ArPd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = w2v_model.wv.vectors\n",
        "word_vectors.shape"
      ],
      "metadata": {
        "id": "Cjypg_CerQ0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar todo el modelo\n",
        "w2v_model.save(\"word2vec.model\")\n",
        "\n",
        "# Guardar sólo los vectores\n",
        "w2v_model.wv.save(\"word2vec.wordvectors\")"
      ],
      "metadata": {
        "id": "qQEYi4j4ii8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'amplia'\n",
        "\n",
        "word_vector = w2v_model.wv[word]\n",
        "\n",
        "dim = w2v_model.wv.vector_size\n",
        "print(f\"palabra: {word}\")\n",
        "zeros = np.where(word_vector==0)[0].shape[0]\n",
        "print(f\"Número de entradas 0: {zeros}/{dim}={round(100*zeros/dim,2)}%\")"
      ],
      "metadata": {
        "id": "TbqJSIOpjjmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos qué pasa con las palabras **OOV**"
      ],
      "metadata": {
        "id": "EEXpowRFLAuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv['holonomia']"
      ],
      "metadata": {
        "id": "KleozY7bLAkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,1))\n",
        "plt.suptitle(\"Word2Vec\")\n",
        "plt.imshow(word_vector.reshape(1,-1))\n",
        "plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.suptitle(\"BOW\")\n",
        "plt.imshow(bow_vector.reshape(27,-1))\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w05oKHmnjsBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grafiquemos la reducción de dimensionalidad 3d t-SNE\n",
        "\n",
        "# from sklearn.manifold import TSNE\n",
        "# import plotly.graph_objects as go\n",
        "# import plotly\n",
        "\n",
        "# vocabulary = w2v_model.wv.index_to_key\n",
        "# word_vectors = w2v_model.wv.vectors\n",
        "\n",
        "# tsne = TSNE(n_components=3, metric='cosine')\n",
        "# X_tsne = tsne.fit_transform(word_vectors)\n",
        "\n",
        "# plotly.offline.init_notebook_mode()\n",
        "\n",
        "# trace = go.Scatter3d(\n",
        "#     x=X_tsne[:,0],\n",
        "#     y=X_tsne[:,1],\n",
        "#     z=X_tsne[:,2],\n",
        "#     mode='markers',\n",
        "#     marker={\n",
        "#         'size': 3,\n",
        "#         'opacity': 0.75,\n",
        "#         'color': 'black'\n",
        "#     },\n",
        "#     hovertemplate='%{text}<extra></extra>',\n",
        "#     text = [f\"{vocabulary[j]}\" for j in range(X_tsne.shape[0])]\n",
        "# )\n",
        "\n",
        "# layout = go.Layout(\n",
        "#     margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        "# )\n",
        "\n",
        "# data = [trace]\n",
        "\n",
        "# plot_figure = go.Figure(data=data, layout=layout)\n",
        "\n",
        "# plot_figure.update_layout(\n",
        "#     title = 'Wikipedia Words',\n",
        "#     scene = dict(\n",
        "#         xaxis = dict(visible=False),\n",
        "#         yaxis = dict(visible=False),\n",
        "#         zaxis =dict(visible=False)\n",
        "#         )\n",
        "#     )\n",
        "\n",
        "# plotly.offline.plot(plot_figure, filename='/content/drive/MyDrive/Colab Notebooks/NLP/Mi curso/wiki-w2v-tsne3d-words.html')"
      ],
      "metadata": {
        "id": "esn2CdjwkG3S",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectores de documentos\n",
        "\n",
        "Algunas técnicas para obtener vectores de documentos:\n",
        "\n",
        "* Promediar los vectores de word2vec. Según Le y Mikolov, este enfoque no funciona bien para tareas de análisis de sentimientos, porque «pierde el orden de las palabras del mismo modo que los modelos estándar de bolsa de palabras» y «no reconoce muchos fenómenos lingüísticos sofisticados, como el sarcasmo». Por otro lado, según Kenter et al. 2016, «promediar simplemente las incrustaciones de palabras de todas las palabras de un texto ha demostrado ser una línea de base o característica sólida en multitud de tareas», como las tareas de similitud de textos cortos.\n",
        "* Ponderar los vectores de palabras con su TF-IDF para disminuir la influencia de las palabras más comunes.\n",
        "* Concatenar los vectores de palabras.\n",
        "\n",
        "Observar que la operación de sumar vectores ignora el orden de las palabras por lo que caemos en una representación tipo BOW."
      ],
      "metadata": {
        "id": "m-sc0NMsotS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim permite obtener un promedio de vectores"
      ],
      "metadata": {
        "id": "XZ4XejQqKvHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vectors = np.array([w2v_model.wv.get_mean_vector(doc) for doc in docs])\n",
        "doc_vectors.shape"
      ],
      "metadata": {
        "id": "FvGnt2f1nuuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.suptitle(\"Normas de los vectores de documentos\")\n",
        "plt.hist(np.linalg.norm(doc_vectors,axis=1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tRqosvSSZ_4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('wikipedia_w2v_mean_doc_vectors.npy',doc_vectors)"
      ],
      "metadata": {
        "id": "uAsccR5zVb6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vectors = np.load('wikipedia_w2v_mean_doc_vectors.npy')"
      ],
      "metadata": {
        "id": "Yd5_FFUKY9jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,z in enumerate(doc_vectors):\n",
        "    doc_vectors[i] = normalizar_vector(z)"
      ],
      "metadata": {
        "id": "kPT7-9zgZWGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grafiquemos la reducción de dimensionalidad 3d UMAP\n",
        "\n",
        "# from umap import UMAP\n",
        "# import matplotlib.pyplot as plt\n",
        "# import plotly\n",
        "# import plotly.graph_objs as go\n",
        "\n",
        "# umap = UMAP(metric='cosine',n_components=3)\n",
        "# X_umap = umap.fit_transform(doc_vectors)\n",
        "\n",
        "# plotly.offline.init_notebook_mode()\n",
        "\n",
        "# trace = go.Scatter3d(\n",
        "#     x=X_umap[:,0],\n",
        "#     y=X_umap[:,1],\n",
        "#     z=X_umap[:,2],\n",
        "#     mode='markers',\n",
        "#     marker={\n",
        "#         'size': 3,\n",
        "#         'opacity': 0.75,\n",
        "#         'color': 'blue'\n",
        "#     },\n",
        "#     hovertemplate='%{text}<extra></extra>',\n",
        "#     text = [f\"{docs_raw[j][:75]}\" for j in range(X_umap.shape[0])]\n",
        "# )\n",
        "\n",
        "# layout = go.Layout(\n",
        "#     margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
        "# )\n",
        "\n",
        "# data = [trace]\n",
        "\n",
        "# plot_figure = go.Figure(data=data, layout=layout)\n",
        "\n",
        "# plot_figure.update_layout(\n",
        "#     title = 'Wikipedia Docs',\n",
        "#     scene = dict(\n",
        "#         xaxis = dict(visible=False),\n",
        "#         yaxis = dict(visible=False),\n",
        "#         zaxis =dict(visible=False)\n",
        "#         )\n",
        "#     )\n",
        "\n",
        "# plotly.offline.plot(plot_figure, filename='/content/drive/MyDrive/Colab Notebooks/NLP/Mi curso/wiki-w2v-norm-umap3d-docs.html')"
      ],
      "metadata": {
        "id": "3IqUxc0yrqXm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un ejemplo de uso"
      ],
      "metadata": {
        "id": "MrOsSbo2y2_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El corpus"
      ],
      "metadata": {
        "id": "CJT3T-pzDQgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "8s5kKKZqFNQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings como features para tareas de Machine Learning"
      ],
      "metadata": {
        "id": "pomHLKPKQeUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 18kGdlhOiQNS61wUK7uPbdquKL3XJrgzf"
      ],
      "metadata": {
        "id": "lCnrIHqCQgc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "imdb_df = pd.read_csv('IMDB.csv')\n",
        "display(imdb_df)\n",
        "\n",
        "y = LabelEncoder().fit_transform(imdb_df['sentiment'].values)\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(imdb_df['review'].values, y, test_size=0.2, random_state=642)"
      ],
      "metadata": {
        "id": "TW6ShS3oQosL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "random_idxs = np.random.choice(imdb_df.shape[0],5,replace=False)\n",
        "\n",
        "for j in random_idxs:\n",
        "    text = imdb_df.loc[j,'review']\n",
        "    sentiment = imdb_df.loc[j,'sentiment']\n",
        "    print(f\"{text[:80]}...:\\n\\t{sentiment}\")\n"
      ],
      "metadata": {
        "id": "iTZT0aUBXnwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El preprocesamiento y limpieza tarda alrededor de 2 minutos"
      ],
      "metadata": {
        "id": "Z0kKkmE4ZUn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_raw = [re.sub(r'\\d+', ' ', doc) for doc in X_train_raw]\n",
        "train_tokenized_docs = [[x for x in word_tokenize(doc) if x not in stopwords and x not in punctuation]\n",
        "                        for doc in X_train_raw]\n",
        "train_docs = [' '.join(doc) for doc in train_tokenized_docs]\n",
        "\n",
        "X_test_raw = [re.sub(r'\\d+', ' ', doc) for doc in X_test_raw]\n",
        "test_tokenized_docs = [[x for x in word_tokenize(doc) if x not in stopwords and x not in punctuation]\n",
        "                       for doc in X_test_raw]\n",
        "test_docs = [' '.join(doc) for doc in test_tokenized_docs]"
      ],
      "metadata": {
        "id": "bMP1l-diUQ7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=2000, stop_words=stopwords)\n",
        "X_train_tfidf = vectorizer.fit_transform(train_docs).toarray()\n",
        "X_test_tfidf = vectorizer.transform(test_docs).toarray()"
      ],
      "metadata": {
        "id": "DDiFFWG0UXHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El modelo de embeddings"
      ],
      "metadata": {
        "id": "kkBNanr1DTYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenemos un modelo de word2vec en el corpus IMDB. **Tarda alrededor de 2 minutos**"
      ],
      "metadata": {
        "id": "IDdikdrab-P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_20ng_model = Word2Vec(sentences=train_tokenized_docs, vector_size=100, window=5, min_count=5, workers=4)"
      ],
      "metadata": {
        "id": "1DUPF4k52ifS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_20ng_model.wv['ball']"
      ],
      "metadata": {
        "id": "3DTbvJ5B6AY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_20ng_model.wv.most_similar('ball',topn=10)"
      ],
      "metadata": {
        "id": "67hGm2516PzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_20ng_model.wv.most_similar('space',topn=10)"
      ],
      "metadata": {
        "id": "E_TOrETZDxWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando el promedio de embeddings de palabras"
      ],
      "metadata": {
        "id": "jpariMZWCjqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* IMDB: 35 minutos"
      ],
      "metadata": {
        "id": "JS4O9vFZhtlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_doc_vectors = np.zeros((len(train_docs), w2v_20ng_model.wv.vector_size))\n",
        "test_doc_vectors = np.zeros((len(test_docs), w2v_20ng_model.wv.vector_size))\n",
        "\n",
        "for i, doc in enumerate(train_docs):\n",
        "    words = [w for w in doc.split() if w in w2v_20ng_model.wv.index_to_key]\n",
        "    if len(words) > 0:\n",
        "        these_vectors = np.array([w2v_20ng_model.wv[w] for w in words])\n",
        "        train_doc_vectors[i] = np.mean(these_vectors, axis=0)\n",
        "print(train_doc_vectors[:3,:5])\n",
        "\n",
        "for i, doc in enumerate(test_docs):\n",
        "    words = [w for w in doc.split() if w in w2v_20ng_model.wv.index_to_key]\n",
        "    if len(words) > 0:\n",
        "        these_vectors = np.array([w2v_20ng_model.wv[w] for w in words])\n",
        "        test_doc_vectors[i] = np.mean(these_vectors, axis=0)"
      ],
      "metadata": {
        "id": "Wqs-K8d52qAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('IMDB_w2v_train_doc_vectors.npy',train_doc_vectors)\n",
        "np.save('IMDB_w2v_test_doc_vectors.npy',test_doc_vectors)"
      ],
      "metadata": {
        "id": "6coMbXbNdj5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1DUJuV6Dl5-eOj6VRgZ-Q27mLk9Gj0GLS\n",
        "!gdown 1lO4I-RiB2Xd6wQmMH7Q5P0qAFMxUrcMW\n",
        "\n",
        "train_doc_vectors = np.load('IMDB_w2v_train_doc_vectors.npy')\n",
        "test_doc_vectors = np.load('IMDB_w2v_test_doc_vectors.npy')"
      ],
      "metadata": {
        "id": "0TqeHCr9ejjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos cuántos vectores nulos hay, **¿por qué podría ocurrir esto?**"
      ],
      "metadata": {
        "id": "FdAo3EWN94Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vectores nulos en el conjunto train: {np.where(np.sum(np.abs(train_doc_vectors),axis=1)==0)[0].shape[0]}\")\n",
        "print(f\"Vectores nulos en el conjunto test: {np.where(np.sum(np.abs(test_doc_vectors),axis=1)==0)[0].shape[0]}\")"
      ],
      "metadata": {
        "id": "-rCMMUnE20jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.suptitle(\"Normas de los vectores de documentos\")\n",
        "plt.hist(np.linalg.norm(train_doc_vectors,axis=1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ag5Qc1wn9s5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probar normalizando y sin normalizar.\n",
        "\n",
        "Entrenamos un modelo de machine learning para la tarea de clasificación usando los embeddings. Evaluamos usando el accuracy.\n",
        "\n",
        "IMDB: Sin normalizar $\\approx$ 82%-85%"
      ],
      "metadata": {
        "id": "OO1n1yUvNfYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clfs = [SVC(), LogisticRegression(),RandomForestClassifier(),\n",
        "        MLPClassifier(hidden_layer_sizes=(50,50))]\n",
        "\n",
        "for clf in clfs:\n",
        "    clf.fit(train_doc_vectors, y_train)\n",
        "    print(clf.score(test_doc_vectors, y_test))"
      ],
      "metadata": {
        "id": "oTxkjAfW_nk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚡ Usando el promedio pesado con tf-idf de palabras\n",
        "\n",
        "Esta es una técnica híbrida para representar documentos, consiste en re-escalar los vectores de las palabras usando como pesos los coeficientes TF-IDF"
      ],
      "metadata": {
        "id": "LVUK7nCBDbqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consideremos ambos vocabularios"
      ],
      "metadata": {
        "id": "gTccY8gfH5D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_w2v = w2v_20ng_model.wv.index_to_key\n",
        "print(len(vocabulary_w2v))\n",
        "print(vocabulary_w2v[:20])\n",
        "\n",
        "vocabulary_tfidf = vectorizer.get_feature_names_out()\n",
        "print(len(vocabulary_tfidf))\n",
        "print(vocabulary_tfidf[:20])"
      ],
      "metadata": {
        "id": "d8GcBfz8_5Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_doc_vectors = np.zeros((len(train_docs), w2v_20ng_model.wv.vector_size))\n",
        "for i, doc in enumerate(train_docs):\n",
        "    words = [w for w in doc.split() if w in vocabulary_w2v and w in vocabulary_tfidf]\n",
        "    if len(words) > 0:\n",
        "        these_weights = np.array([X_train_tfidf[i,vectorizer.vocabulary_[w]] for w in words])\n",
        "        these_vectors = np.array([w2v_20ng_model.wv[w] for w in words])\n",
        "        train_doc_vectors[i] = np.sum(these_vectors * these_weights.reshape(-1,1), axis=0)\n",
        "\n",
        "test_doc_vectors = np.zeros((len(test_docs), w2v_20ng_model.wv.vector_size))\n",
        "for i, doc in enumerate(test_docs):\n",
        "    words = [w for w in doc.split() if w in vocabulary_w2v and w in vocabulary_tfidf]\n",
        "    if len(words) > 0:\n",
        "        these_weights = np.array([X_test_tfidf[i,vectorizer.vocabulary_[w]] for w in words])\n",
        "        these_vectors = np.array([w2v_20ng_model.wv[w] for w in words])\n",
        "        test_doc_vectors[i] = np.sum(these_vectors * these_weights.reshape(-1,1), axis=0)"
      ],
      "metadata": {
        "id": "eTuHe9gdDmom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('imdb_w2v_tfidf_train_doc_vectors.npy',train_doc_vectors)\n",
        "np.save('imdb_w2v_tfidf_test_doc_vectors.npy',test_doc_vectors)"
      ],
      "metadata": {
        "id": "kLV9DlE7q1Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/Colab Notebooks/imdb_w2v_tfidf_train_doc_vectors.npy',train_doc_vectors)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/imdb_w2v_tfidf_test_doc_vectors.npy',test_doc_vectors)"
      ],
      "metadata": {
        "id": "zCm5YZ8E15Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vectores nulos en el conjunto train: {np.where(np.sum(np.abs(train_doc_vectors),axis=1)==0)[0].shape[0]}\")\n",
        "print(f\"Vectores nulos en el conjunto test: {np.where(np.sum(np.abs(test_doc_vectors),axis=1)==0)[0].shape[0]}\")"
      ],
      "metadata": {
        "id": "TyxG1VDvHgh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.suptitle(\"Normas de los vectores de documentos\")\n",
        "plt.hist(np.linalg.norm(train_doc_vectors,axis=1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dMnIu80CHwNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,v in enumerate(train_doc_vectors):\n",
        "    train_doc_vectors[i] = normalizar_vector(v)\n",
        "\n",
        "for i,v in enumerate(test_doc_vectors):\n",
        "    test_doc_vectors[i] = normalizar_vector(v)\n",
        "\n",
        "plt.figure()\n",
        "plt.suptitle(\"Normas de los vectores de documentos\")\n",
        "plt.hist(np.linalg.norm(train_doc_vectors,axis=1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C_iTt__5Ni5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* IMDB: Sin normalizar, 79-82%. Normalizando, 81-82%"
      ],
      "metadata": {
        "id": "p25ouZd6H4KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clfs = [SVC(), LogisticRegression(),\n",
        "        RandomForestClassifier(),\n",
        "        MLPClassifier(hidden_layer_sizes=(50,50))]\n",
        "\n",
        "for clf in clfs:\n",
        "    clf.fit(train_doc_vectors, y_train)\n",
        "    print(clf.score(test_doc_vectors, y_test))"
      ],
      "metadata": {
        "id": "njgIwJq9HyVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PkbqnTdmH1eX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}