{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-MCD/blob/main/08-TopicDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfl1iJM3hBfz"
      },
      "source": [
        "<h1>Topic Detection</h1>\n",
        "\n",
        "<h2>Topic Modelling</h1>\n",
        "\n",
        "En esta notebook ahondaremos un poco m√°s en la tarea de *Topic Detection*. Para esto usaremos varias t√©cnicas, algunas de ellas nuevas:\n",
        "\n",
        "* Clustering en representaciones vectoriales de documentos.\n",
        "* LDA. Implementaci√≥n en [gensim](https://radimrehurek.com/gensim/models/ldamodel.html).\n",
        "* LSA. Implementaci√≥n en [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
        "\n",
        "Adem√°s, evaluaremos estas tareas usando m√©tricas propias de la tarea.\n",
        "* [Coherence](https://radimrehurek.com/gensim/models/coherencemodel.html): [source](https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB8amIsHeRq6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "import spacy\n",
        "\n",
        "from gensim import models, corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "m8XXyGjnsPFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consideraremos un corpus de 92579 documentos de texto, consisten en noticias de CNN. **Origen desconocido**"
      ],
      "metadata": {
        "id": "lKjlglousGjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1S-KYaCpb39vMphrkdnceXUkUhzHeapt7"
      ],
      "metadata": {
        "id": "IDX_mqfTbjS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cnn_articles.txt', 'r', encoding='utf8') as f:\n",
        "    articles = f.read().split('@delimiter')\n",
        "\n",
        "print(len(articles))\n",
        "\n",
        "cnn_df = pd.DataFrame({'document':articles})\n",
        "cnn_df"
      ],
      "metadata": {
        "id": "2yPsxYW8bqyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos quedamos solamente con los primeros 10000 art√≠culos."
      ],
      "metadata": {
        "id": "I0jlQZhPcntH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lovGKCkqY_3w"
      },
      "outputs": [],
      "source": [
        "corpus = articles[:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploremos los documentos, aqu√≠ podemos ver algunas palabras que podemos a√±adir a la lista de stopwords."
      ],
      "metadata": {
        "id": "gUay3dT4sinY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "wc = WordCloud(background_color=\"white\", max_words=2000, contour_width=3, contour_color='steelblue')\n",
        "wordcloud = wc.generate(\" \".join(corpus))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ij9Dgj4RsU41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos el preprocesamiento usando un pipeline *ligero* de scipy. Tarda alrededor de 40s."
      ],
      "metadata": {
        "id": "A-D08fNPczo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "\n",
        "tokenized_docs = [[t.text for t in tok_doc if\n",
        "          not t.is_punct and \\\n",
        "          not t.is_space and \\\n",
        "          t.is_alpha] for tok_doc in nlp.pipe(corpus) ]"
      ],
      "metadata": {
        "id": "v_eaJKcvctVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.extend(['said'])"
      ],
      "metadata": {
        "id": "j42JNQhcs90F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs = [[w for w in doc if w not in stopwords] for doc in tokenized_docs]"
      ],
      "metadata": {
        "id": "4Zj0sMSDssuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNb30xLnMOIM"
      },
      "outputs": [],
      "source": [
        "print(tokenized_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë Si usamos un pipeline m√°s completo podemos tardar hasta 12 minutos."
      ],
      "metadata": {
        "id": "aE23aP6uc9z5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bthrWkCXzbQG"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
        "\n",
        "# tokenized_docs = [[t.text for t in tok_doc if\n",
        "#           not t.is_punct and \\\n",
        "#           not t.is_space and \\\n",
        "#           t.is_alpha] for tok_doc in nlp.pipe(corpus) ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSA con scikit-learn"
      ],
      "metadata": {
        "id": "4vuZVSehdfln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\" \".join(doc) for doc in tokenized_docs]\n",
        "\n",
        "print(docs[:3])\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                            max_features= 1000,\n",
        "                            smooth_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(docs)\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "id": "BTXUYis1djUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd_model = TruncatedSVD(n_components=10, algorithm='randomized',\n",
        "                         n_iter=100, random_state=122)\n",
        "\n",
        "svd_model.fit(X)\n",
        "len(svd_model.components_)"
      ],
      "metadata": {
        "id": "cak039hgd-EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd_model.components_.shape"
      ],
      "metadata": {
        "id": "tpm79cxSjLGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "for i, comp in enumerate(svd_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "    print(f\"Topic {str(i)}: \")\n",
        "    for t in sorted_terms:\n",
        "        print(t[0], end=', ')\n",
        "    print()"
      ],
      "metadata": {
        "id": "ZSwmXHsFeV-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd_model.transform(X).shape"
      ],
      "metadata": {
        "id": "p7U8pGXdtc6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = np.argmax(svd_model.transform(X), axis=1)\n",
        "num_topics = len(np.unique(topics))\n",
        "\n",
        "docs_idxs_per_topic = [np.where(topics == i)[0] for i in range(num_topics)]\n",
        "\n",
        "fig, axs = plt.subplots(1, num_topics, figsize=(15, 15))\n",
        "for ax,j in zip(axs.flatten(),range(num_topics)):\n",
        "    topic_docs = \" \".join([docs[i] for i in docs_idxs_per_topic[j]])\n",
        "    wc = WordCloud(background_color=\"white\", max_words=2000).generate(topic_docs)\n",
        "    ax.imshow(wc, interpolation='bilinear')\n",
        "    ax.set_title(f\"Topic {j}\")\n",
        "    ax.axis(\"off\")\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "G0FeghC88WpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_umass_score(dt_matrix, i, j):\n",
        "    zo_matrix = (dt_matrix > 0).astype(int)\n",
        "    col_i, col_j = zo_matrix[:, i], zo_matrix[:, j]\n",
        "    col_ij = col_i + col_j\n",
        "    col_ij = (col_ij == 2).astype(int)\n",
        "    Di, Dij = col_i.sum(), col_ij.sum()\n",
        "    return math.log((Dij + 1) / Di)\n",
        "\n",
        "def get_topic_coherence(dt_matrix, topic, n_top_words):\n",
        "    indexed_topic = zip(topic, range(0, len(topic)))\n",
        "    topic_top = sorted(indexed_topic, key=lambda x: 1 - x[0])[0:n_top_words]\n",
        "    coherence = 0\n",
        "    for j_index in range(0, len(topic_top)):\n",
        "        for i_index in range(0, j_index - 1):\n",
        "            i = topic_top[i_index][1]\n",
        "            j = topic_top[j_index][1]\n",
        "            coherence += get_umass_score(dt_matrix, i, j)\n",
        "    return coherence\n",
        "\n",
        "def get_average_topic_coherence(dt_matrix, topics, n_top_words):\n",
        "    total_coherence = 0\n",
        "    for i in range(0, len(topics)):\n",
        "        total_coherence += get_topic_coherence(dt_matrix, topics[i], n_top_words)\n",
        "    return total_coherence / len(topics)"
      ],
      "metadata": {
        "id": "tXZPuWEo4rUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_average_topic_coherence(X, svd_model.components_, 10)"
      ],
      "metadata": {
        "id": "i8egQYgF44hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_topics = [3,5,7,10,15,20]\n",
        "coherences = []\n",
        "\n",
        "for k in num_topics:\n",
        "    svd_model = TruncatedSVD(n_components=k, algorithm='randomized',\n",
        "                         n_iter=100, random_state=122)\n",
        "    svd_model.fit(X)\n",
        "    coherences.append(get_average_topic_coherence(X, svd_model.components_, 10))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(num_topics, coherences)\n",
        "plt.xlabel('Number of topics')\n",
        "plt.ylabel('Coherence')\n",
        "plt.xticks(num_topics)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d2Xn76p96rzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [LDA](https://radimrehurek.com/gensim/models/ldamodel.html) con gensim"
      ],
      "metadata": {
        "id": "g8GTMb0VdksA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para usar la implementaci√≥n de LDA de gensim necesitamos un diccionario relacionando los √≠ndices de las palabras y las palabras. Esta informaci√≥n ya la tenemos con el vectorizador TF-IDF.\n",
        "\n",
        "El atributo `vocabulary_` de la clase `TfidfVectorizer` es un diccionario de la forma\n",
        "            \n",
        "        word: word_index"
      ],
      "metadata": {
        "id": "nN56GMxYjRGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "id": "I-xUt0qQe31j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el modelo LDA de gensim necesitamos especificar un diccionario de la forma\n",
        "            \n",
        "        word_index: word"
      ],
      "metadata": {
        "id": "nKs1bA3rjEBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dicc = dict(zip(vectorizer.vocabulary_.values(),vectorizer.vocabulary_.keys()))"
      ],
      "metadata": {
        "id": "JT1y6NDCfOq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necesitamos tambi√©n especificar una matriz sparse de scipy."
      ],
      "metadata": {
        "id": "wI00xtNulhoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "X_csc = csr_matrix(X)"
      ],
      "metadata": {
        "id": "irMKWPHIgo1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from gensim.matutils import Sparse2Corpus\n",
        "\n",
        "lda_model = models.LdaModel(corpus=Sparse2Corpus(X_csc,documents_columns=False), num_topics=10, id2word=dicc, random_state=1)"
      ],
      "metadata": {
        "id": "ty1N7M92e1bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.get_topics().shape"
      ],
      "metadata": {
        "id": "BLLmM0wtfjsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_id = 784\n",
        "\n",
        "word_topics = lda_model.get_term_topics(word_id=word_id,minimum_probability=0)\n",
        "word_topics = sorted(word_topics,key=lambda x: x[1], reverse=True)\n",
        "print(f\"Word: {dicc[word_id]}\")\n",
        "print(f\"Topics: {word_topics}\")"
      ],
      "metadata": {
        "id": "ZtJZBkaUi5xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqGjDaKbPL6_"
      },
      "outputs": [],
      "source": [
        "lda_model.print_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos inspeccionar m√°s a detalle un t√≥pico"
      ],
      "metadata": {
        "id": "MAVhFl9rn760"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.show_topic(topicid=9, topn=15)"
      ],
      "metadata": {
        "id": "GAV5kXJUn2yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'team'\n",
        "word_id = vectorizer.vocabulary_[word]\n",
        "\n",
        "word_topics = lda_model.get_term_topics(word_id=word_id,minimum_probability=0)\n",
        "word_topics = sorted(word_topics,key=lambda x: x[1], reverse=True)\n",
        "print(f\"Word: {word}\")\n",
        "print(f\"Topics: {word_topics}\")"
      ],
      "metadata": {
        "id": "2Tv53rnsmFxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.show_topics(num_topics=10, num_words=10, log=False, formatted=True)"
      ],
      "metadata": {
        "id": "BJYOmOxTnptm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "dictionary = Dictionary.from_corpus(Sparse2Corpus(X_csc,documents_columns=False), id2word=dicc)\n",
        "bow = dictionary.doc2bow(tokenized_docs[0])\n",
        "\n",
        "topics = lda_model.get_document_topics(bow=bow, minimum_probability=None)\n",
        "\n",
        "print(topics)"
      ],
      "metadata": {
        "id": "yQwTWIU_xKFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los t√≥picos de todos los documentos"
      ],
      "metadata": {
        "id": "D5lRL3Z1Alld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics_list = []\n",
        "\n",
        "for doc in tokenized_docs:\n",
        "    bow = dictionary.doc2bow(doc)\n",
        "    topics = lda_model.get_document_topics(bow=bow, minimum_probability=None)\n",
        "    topics = sorted(topics,key=lambda x: x[1], reverse=True)\n",
        "    topics_list.append(topics[0])\n",
        "\n",
        "print(topics_list[:10])"
      ],
      "metadata": {
        "id": "b4angQJIAkj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = np.array([x[0] for x in topics_list])\n",
        "num_topics = len(np.unique(topics))\n",
        "\n",
        "docs_idxs_per_topic = [np.where(topics == i)[0] for i in range(num_topics)]\n",
        "\n",
        "fig, axs = plt.subplots(1, num_topics, figsize=(15, 15))\n",
        "for ax,j in zip(axs.flatten(),range(num_topics)):\n",
        "    topic_docs = \" \".join([docs[i] for i in docs_idxs_per_topic[j]])\n",
        "    wc = WordCloud(background_color=\"white\", max_words=2000).generate(topic_docs)\n",
        "    ax.imshow(wc, interpolation='bilinear')\n",
        "    ax.set_title(f\"Topic {j}\")\n",
        "    ax.axis(\"off\")\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "snQqqo3sJcHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluaci√≥n\n",
        "\n",
        "La coherencia mide la distancia relativa entre palabras dentro de un t√≥pico. Hay dos tipos principales:\n",
        "* `c_v` t√≠picamente est√° en $0 < x < 1$.\n",
        "* `u_mass` t√≠picamente es negativo.\n",
        "\n",
        "Valores m√°s altos son mejores.\n",
        "\n",
        "The coherence of a topic is regarded as the sum of pairwise distributional similarity scores over the set of topic words, V:\n",
        "\n",
        "$$\\text{coh}(V) = \\sum_{v_i,v_j \\in V} \\text{score}(v_i,v_j,…õ).$$\n",
        "\n",
        "* `uci` es extr√≠nseca, los conteos se hacen en corpus externos.\n",
        "$$\\text{score}(v_i,v_j,…õ) = \\log\\frac{p(w_i,w_j) + …õ}{p(w_i)p(w_j)}$$\n",
        "\n",
        "* `u_mass` en intr√≠nseca, los conteos se hacen en corpus internos y no es sim√©trica.\n",
        "$$\\text{score}(v_i,v_j,…õ) = \\log\\frac{D(w_i,w_j) + …õ}{D(w_i)}$$\n",
        "\n",
        "Referencias: [art√≠culo original coherencia UMASS](https://aclanthology.org/D11-1024.pdf), [art√≠culo comparando UMASS & UCI](https://aclanthology.org/D12-1087.pdf), [discusi√≥n en github](https://github.com/piskvorky/gensim/pull/710#issuecomment-425344644), [otra referencia](https://qpleple.com/topic-coherence-to-evaluate-topic-models/)."
      ],
      "metadata": {
        "id": "6-JBbJnmoIQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = CoherenceModel(model=lda_model,\n",
        "                    coherence='u_mass',\n",
        "                    corpus=Sparse2Corpus(X_csc,documents_columns=False),\n",
        "                    )\n",
        "coherence = cm.get_coherence()\n",
        "coherence"
      ],
      "metadata": {
        "id": "N79ZT3y-vwTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos usar las estrategias: `c_v`, `c_uci`, `c_npmi` tenemos que proporcionar informaci√≥n del corpus."
      ],
      "metadata": {
        "id": "79rNFU4mv5U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "dictionary = Dictionary.from_corpus(Sparse2Corpus(X_csc,documents_columns=False), id2word=dicc)\n",
        "\n",
        "cm = CoherenceModel(model=lda_model,\n",
        "                    corpus=Sparse2Corpus(X_csc,documents_columns=False),\n",
        "                    coherence='c_v',\n",
        "                    texts=tokenized_docs,\n",
        "                    dictionary=dictionary)\n",
        "coherence = cm.get_coherence()\n",
        "coherence"
      ],
      "metadata": {
        "id": "b40xCy9SoJl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi√©n podemos ver la coherencia por t√≥pico:"
      ],
      "metadata": {
        "id": "uCBPUz0vxArh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm.get_coherence_per_topic()"
      ],
      "metadata": {
        "id": "SqjbxGUqt8IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos decidir el n√∫mero de t√≥picos en funci√≥n del valor de coherencia"
      ],
      "metadata": {
        "id": "SpcKYX6QA5LJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LePgWgStnR-B"
      },
      "outputs": [],
      "source": [
        "num_topics = [3+k for k in range(15)]\n",
        "coherences = []\n",
        "\n",
        "for k in num_topics:\n",
        "    lda_model = models.LdaModel(corpus=Sparse2Corpus(X_csc,documents_columns=False),\n",
        "                                num_topics=k, id2word=dicc, random_state=1)\n",
        "    cm = CoherenceModel(model=lda_model,\n",
        "                    coherence='u_mass',\n",
        "                    corpus=Sparse2Corpus(X_csc,documents_columns=False),\n",
        "                    )\n",
        "    coherences.append(cm.get_coherence())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExuVMgK1PwcW"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(num_topics, coherences)\n",
        "plt.xlabel('Number of topics')\n",
        "plt.ylabel('Coherence')\n",
        "plt.xticks(num_topics)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20newsgroups\n",
        "\n",
        "Podemos repetir el experimento con el corpus `20newsgroups`"
      ],
      "metadata": {
        "id": "pH8hARr3yyl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "effQ5DqyfXTG"
      },
      "outputs": [],
      "source": [
        "train_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "test_data = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_train = train_data.data\n",
        "X_test = test_data.data\n",
        "\n",
        "y_train = train_data.target\n",
        "y_test = test_data.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJY5R0uXiiJG"
      },
      "outputs": [],
      "source": [
        "news_df = pd.DataFrame({'document':X_train,\n",
        "                        'topic': y_train})\n",
        "news_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü• Tarea\n",
        "\n",
        "## Introducci√≥n\n",
        "\n",
        "Si tenemos un etiquetado ground truth podemos usar m√©tricas que comparan entre agrupamientos. Algunas de estas m√©tricas suelen usarse en tareas de clustering. Algunos ejemplos son:\n",
        "\n",
        "1. [Rand Index **RI**](https://scikit-learn.org/stable/modules/clustering.html#rand-index)\n",
        "2. [Mutual Information based scores **MI**](https://scikit-learn.org/stable/modules/clustering.html#mutual-information-based-scores)\n",
        "3. [Homogeneity, completeness and V-measure **HCV**](https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure)\n",
        "\n",
        "## Ejercicios\n",
        "\n",
        "Vamos a realizar la tarea de topic modeling usando el corpus `20newsgroups`\n",
        "\n",
        "1. Usando LDA obten 20 t√≥picos, mide el desempe√±o usando una m√©trica de cada uno de los 3 grupos descritos anteriormentes (RI, MI, HCV). Tambi√©n mide el desempe√±o usando la coherencia.\n",
        "2. Usando LSA obten 20 t√≥picos, mide el desempe√±o usando una m√©trica de cada uno de los 3 grupos descritos anteriormentes (RI, MI, HCV).\n",
        "3. Usando un algoritmo de clustering donde se especifique el n√∫mero de clusters, obtener 20 clusters que, idealmente, representen los t√≥picos. El algoritmo de clustering lo aplicaras a las representaciones BOW o TF-IDF. Escoge la que mejor desempe√±o tenga de acuerdo a alguna de las m√©tricas de los 3 grupos anteriores (RI, MI, HCV).\n",
        "\n",
        "4. En cada una de las 3 estrategias haz una exploraci√≥n manual de algunos documentos, ¬øparece haber coherencia?\n",
        "5. En cada una de las 3 estrategias haz una nube de palabras por cada t√≥pico, ¬øparece haber coherencia en el vocabulario?\n",
        "\n",
        "## Conclusiones\n",
        "\n",
        "Redacta un peque√±o texto respondiendo las siguientes preguntas.\n",
        "\n",
        "* ¬øCu√°l m√©todo produjo mejores resultados en este caso?\n",
        "* El hecho de tener las etiquetas reales de t√≥picos, ¬øfacilita la tarea?"
      ],
      "metadata": {
        "id": "l5fU2-34gJpR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_lM3iq2iBR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYU1buHhWEidJf00GN6jAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}