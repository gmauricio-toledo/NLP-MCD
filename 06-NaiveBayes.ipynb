{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNR4yA9rfFNVwfHDEz6C0tT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmauricio-toledo/NLP-MCD/blob/main/06-NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Naive Bayes</h1>"
      ],
      "metadata": {
        "id": "Z_11reVW_VJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta notebook usaremos el algoritmo Naive Bayes para tareas de clasificaci√≥n de texto.\n",
        "\n",
        "Para esto, usaremos el corpus `20newsgroups`.\n",
        "\n",
        "Algunos aspectos importantes a observar:\n",
        "\n",
        "* Comparaci√≥n del desempe√±o del NaiveBayes como algoritmo de clasificaci√≥n con otros algoritmos.\n",
        "* Interpretabilidad del modelo: palabras m√°s representativas por clase."
      ],
      "metadata": {
        "id": "GNnPXKQw_otO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq wordcloud"
      ],
      "metadata": {
        "id": "8yQeJIJRWXQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rofzguPb_L29"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# El corpus y limpieza"
      ],
      "metadata": {
        "id": "23pHFxfK1YFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el corpus. Nos fijaremos en dos categorias: *space* y *baseball*."
      ],
      "metadata": {
        "id": "T9uY2pjphk7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = fetch_20newsgroups(subset='train',\n",
        "                                remove=('headers', 'footers', 'quotes'),\n",
        "                                categories=['sci.space', 'rec.sport.baseball']\n",
        "                                )\n",
        "test_data = fetch_20newsgroups(subset='test',\n",
        "                                remove=('headers', 'footers', 'quotes'),\n",
        "                                categories=['sci.space', 'rec.sport.baseball']\n",
        "                               )\n",
        "\n",
        "X_train_raw = train_data.data\n",
        "y_train = train_data.target\n",
        "\n",
        "X_test_raw = test_data.data\n",
        "y_test = test_data.target"
      ],
      "metadata": {
        "id": "_zNLU-xDAsS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase negativa es *baseball* y la clase positiva es *space*."
      ],
      "metadata": {
        "id": "svfMZLk3qFTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names_dict = dict(zip([0,1],train_data.target_names))\n",
        "class_names_dict"
      ],
      "metadata": {
        "id": "2lUDgNTBAzJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "X_train_raw = [x.lower() for x in X_train_raw]\n",
        "X_train_raw = [re.sub(r'\\d', ' ', x) for x in X_train_raw]\n",
        "X_train_raw = [re.sub(f'[^a-z ]','',x) for x in X_train_raw]\n",
        "\n",
        "X_test_raw = [x.lower() for x in X_test_raw]\n",
        "X_test_raw = [re.sub(r'\\d', ' ', x) for x in X_test_raw]\n",
        "X_test_raw = [re.sub(f'__',' ',x) for x in X_test_raw]"
      ],
      "metadata": {
        "id": "xEPvciDNIgCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo BOW"
      ],
      "metadata": {
        "id": "BIHhodiL1elC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtengamos los conteos con el modelo `BOW`."
      ],
      "metadata": {
        "id": "UEw_IYYejbXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(max_features=2000,stop_words='english')\n",
        "X_train_bow = cv.fit_transform(X_train_raw)\n",
        "X_test_bow = cv.transform(X_test_raw)\n",
        "\n",
        "print(X_train_bow.shape)\n",
        "print(X_test_bow.shape)"
      ],
      "metadata": {
        "id": "lhGQuLDgA9v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.get_feature_names_out()"
      ],
      "metadata": {
        "id": "yNCwRDJZNtAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "98M0Lp6835iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clasificador Naive Bayes"
      ],
      "metadata": {
        "id": "YylNLTZg1hHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instanciamos y entrenamos el clasificador [Naive Bayes Multinomial](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)."
      ],
      "metadata": {
        "id": "0dUI1LpXjifF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_bow, y_train)"
      ],
      "metadata": {
        "id": "PLMAi5N_3rZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos las predicciones y evaluamos el desempe√±o del modelo"
      ],
      "metadata": {
        "id": "tdM_roBpjw6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nb.predict(X_test_bow)\n",
        "\n",
        "nb_acc = accuracy_score(y_test, y_pred)\n",
        "nb_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ybsVlV2R38Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accedemos a las probabilidades logaritmicas de cada palabra, en cada clase."
      ],
      "metadata": {
        "id": "F96UqLHUj252"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb.feature_log_prob_.shape"
      ],
      "metadata": {
        "id": "hX8v1yrK39tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Definimos una funci√≥n que nos muestre las palabras m√°s determinantes en cada clase\n",
        "\n",
        "def show_top_words(classifier, vectorizer, categories, top_n):\n",
        "  feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
        "  for i, category in enumerate(categories):\n",
        "    prob_sorted = classifier.feature_log_prob_[i, :].argsort()[::-1]\n",
        "    print(f\"{category}:\\n\\t{' | '.join(feature_names[prob_sorted[:top_n]])}\")"
      ],
      "metadata": {
        "id": "2mxYnzXy4NxY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las palabras m√°s determinantes en cada clase, en orden:\n",
        "\n",
        "**Son las palabras con probabilidad m√°s alta de ocurrir en cada clase**"
      ],
      "metadata": {
        "id": "13dKJ7K_kCRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_top_words(nb, cv, train_data.target_names, 12)"
      ],
      "metadata": {
        "id": "rvmj2Bo9-2JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Funci√≥n para colorear todas las palabras del mismo color\n",
        "\n",
        "def frequency_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
        "    '''\n",
        "    Funci√≥n que regresa el color gris\n",
        "    '''\n",
        "    return \"hsl(0,0%, 40%)\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "76Tewu2EiDN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las palabras m√°s frecuentes por clase:"
      ],
      "metadata": {
        "id": "Z8H2WH5fcClg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseball_docs = [x for x, y in zip(X_train_raw, y_train) if y == 0]\n",
        "space_docs = [x for x, y in zip(X_train_raw, y_train) if y == 1]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2,figsize=(6,3),dpi=300)\n",
        "fig.suptitle(\"Most frequent words per class\")\n",
        "fig.subplots_adjust(hspace=0.5,wspace=0.3)\n",
        "# Wordcloud de documentos de baseball:\n",
        "wordcloud = WordCloud(width = 1000, height = 1000,\n",
        "                    background_color ='white',\n",
        "                      min_font_size = 10).generate(' '.join(baseball_docs))\n",
        "axs[0].set_title(\"Baseball\")\n",
        "axs[0].imshow(wordcloud.recolor(color_func=frequency_color_func))\n",
        "axs[0].axis(\"off\")\n",
        "# Wordcloud de documentos del espacio:\n",
        "wordcloud = WordCloud(width = 1000, height = 1000,\n",
        "                    background_color ='white',\n",
        "                    min_font_size = 10).generate(' '.join(space_docs))\n",
        "axs[1].set_title(\"Space\")\n",
        "axs[1].imshow(wordcloud.recolor(color_func=frequency_color_func))\n",
        "axs[1].axis(\"off\")\n",
        "axs[1].set_title(\"Space\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "t8PhEsYqWVd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las palabras con las probabilidades m√°s altas por cada clase:"
      ],
      "metadata": {
        "id": "ZnAcf-xVcJDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs = np.transpose(np.exp(nb.feature_log_prob_))\n",
        "\n",
        "probs_neg = dict(zip(cv.get_feature_names_out(),probs[:,0]))\n",
        "probs_pos = dict(zip(cv.get_feature_names_out(),probs[:,1]))\n",
        "\n",
        "fig, axs = plt.subplots(1, 2,figsize=(6,3),dpi=300)\n",
        "fig.suptitle(\"Most important words per class\")\n",
        "fig.subplots_adjust(hspace=0.5,wspace=0.3)\n",
        "# Wordcloud de documentos de baseball:\n",
        "wordcloud = WordCloud(width = 1000, height = 1000,\n",
        "                    background_color ='white',\n",
        "                      min_font_size = 10).generate_from_frequencies(probs_neg)\n",
        "axs[0].set_title(\"Baseball\")\n",
        "axs[0].imshow(wordcloud.recolor(color_func=frequency_color_func))\n",
        "axs[0].axis(\"off\")\n",
        "# Wordcloud de documentos del espacio:\n",
        "wordcloud = WordCloud(width = 1000, height = 1000,\n",
        "                    background_color ='white',\n",
        "                    min_font_size = 10).generate_from_frequencies(probs_pos)\n",
        "axs[1].set_title(\"Space\")\n",
        "axs[1].imshow(wordcloud.recolor(color_func=frequency_color_func))\n",
        "axs[1].axis(\"off\")\n",
        "axs[1].set_title(\"Space\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FMPHh0VvcI10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probemos con un documento de prueba ajeno:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Main_Page"
      ],
      "metadata": {
        "id": "n7x-aOwDA9M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = '''\n",
        "First Horizon Park, formerly known as First Tennessee Park, is a baseball park in downtown Nashville, Tennessee, United States. The home of the Triple-A Nashville Sounds of the International League, it opened on April 17, 2015, and can seat up to 10,000 people. It replaced the Sounds' former home, Herschel Greer Stadium, where the team played from its founding in 1978 through 2014.\n",
        "\n",
        "The park was built on the site of the former Sulphur Dell, a minor league ballpark in use from 1885 to 1963. It is located between Third and Fifth Avenues on the east and west (home plate, the pitcher's mound, and second base are directly in line with Fourth Avenue to the stadium's north and south) and between Junior Gilliam Way and Harrison Street on the north and south. The Nashville skyline can be seen from the stadium to the south.\n",
        "\n",
        "The design of the park incorporates elements of Nashville's baseball and musical heritage and the use of imagery inspired by Sulphur Dell, the city's former baseball players and teams, and country music. Its most distinctive feature is its guitar-shaped scoreboard‚Äîa successor to the original guitar scoreboard at Greer Stadium. The ballpark's wide concourse wraps entirely around the stadium and provides views of the field from every location.\n",
        "\n",
        "Though primarily a venue for the Nashville Sounds, collegiate and high school baseball teams based in the area, such as the Vanderbilt Commodores and Belmont Bruins, have played some games at the ballpark. Nashville SC, a soccer team of the United Soccer League Championship, played its home matches at the facility from 2018 to 2019. It has also hosted other events, including celebrity softball games and various food and drink festivals.\n",
        "'''\n",
        "\n",
        "cv_new_text = cv.transform([new_text])\n",
        "preds = nb.predict(cv_new_text)\n",
        "print(class_names_dict[preds[0]])"
      ],
      "metadata": {
        "id": "gVZLoEZdpmto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes + Tf-Idf"
      ],
      "metadata": {
        "id": "7cTK6D7iS4Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = CountVectorizer(max_features=2000,stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
        "\n",
        "tfidf_nb = MultinomialNB()\n",
        "tfidf_nb.fit(X_train_tfidf, y_train)\n",
        "y_pred = tfidf_nb.predict(X_test_tfidf)\n",
        "\n",
        "tfidf_nb_acc = accuracy_score(y_test, y_pred)\n",
        "tfidf_nb_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "NKmY8lWPS39-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Otros clasificadores"
      ],
      "metadata": {
        "id": "buJqXttCD6IM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "QDFgs5biD8pJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un clasificador *baseline* y comparemos el rendimiento"
      ],
      "metadata": {
        "id": "y254gAg_kKti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import lru_cache\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_bow, y_train)\n",
        "y_pred = lr.predict(X_test_bow)\n",
        "\n",
        "lru_acc = accuracy_score(y_test, y_pred)\n",
        "lru_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "hKNhlkCvK1h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las palabras m√°s determinantes para predecir la clase positiva (space). Comparemos con el Naive Bayes."
      ],
      "metadata": {
        "id": "R_J3wU_fkSfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fts_importance = lr.coef_.reshape(-1,)\n",
        "\n",
        "sorted(zip(fts_importance, cv.get_feature_names_out()),key=lambda x: x[0], reverse=True)[:10]"
      ],
      "metadata": {
        "id": "sG9atO91LTPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(zip(fts_importance, cv.get_feature_names_out()),key=lambda x: x[0])[:10]"
      ],
      "metadata": {
        "id": "pZNTbevtJP9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "HNqevj2akllY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC()\n",
        "svm.fit(X_train_bow, y_train)\n",
        "y_pred = svm.predict(X_test_bow)\n",
        "\n",
        "svm_acc = accuracy_score(y_test, y_pred)\n",
        "svm_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "oPW54LSVLZHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Con embeddings generados por un modelo de lenguaje\n",
        "\n",
        "Entrenemos un modelo usando embeddings de un modelo basado en transformadores, [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)."
      ],
      "metadata": {
        "id": "6GkU-twuk0fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1qvEuXjb65m6Vh5gn6-XScqtVQz9b1Gme\n",
        "!gdown 14grKYRMZs96xd-tzEvDpEFc4FBO6Vm2H"
      ],
      "metadata": {
        "id": "_A8RSyceoaYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings = np.load(\"imdb-train-space_baseball-roberta.npy\")\n",
        "test_embeddings = np.load(\"imdb-test-space_baseball-roberta.npy\")\n",
        "\n",
        "print(train_embeddings.shape)\n",
        "print(test_embeddings.shape)"
      ],
      "metadata": {
        "id": "k0wFa3I1k003"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(C=2.5)\n",
        "svm.fit(train_embeddings, y_train)\n",
        "y_pred = svm.predict(test_embeddings)\n",
        "\n",
        "svm_emb_acc = accuracy_score(y_test, y_pred)\n",
        "svm_emb_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "bK6BK7DNlFNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparaci√≥n de los desempe√±os"
      ],
      "metadata": {
        "id": "pVQlZDZAPbLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['Naive Bayes + BOW', 'Logistic Regression + BOW', 'SVM + BOW',\n",
        "         'SVM + RoBERTa embeddings', 'Naive Bayes + Tf-Idf']\n",
        "accs = [nb_acc, lru_acc, svm_acc, svm_emb_acc, tfidf_nb_acc]\n",
        "f1s = [nb_f1, lru_f1, svm_f1, svm_emb_f1, tfidf_nb_f1]\n",
        "\n",
        "y = np.arange(len(names))\n",
        "height = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.barh(y - height/2, accs, height, label='Accuracy', color='#3300FF')\n",
        "ax.barh(y + height/2, f1s, height, label='F1 Score', color='#FF00FF')\n",
        "\n",
        "ax.set_yticks(y)\n",
        "ax.set_yticklabels(names)\n",
        "ax.invert_yaxis()  # Labels read top-to-bottom\n",
        "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
        "ax.set_title('Perfomance comparison')\n",
        "ax.legend(loc='best')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Enahdo4UQ-IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Con $n$-gramas"
      ],
      "metadata": {
        "id": "sIi2fIj6TZgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_grams = CountVectorizer(max_features=2000,stop_words='english', ngram_range=(2,3))\n",
        "X_train_tfidf_grams = cv_grams.fit_transform(X_train_raw)\n",
        "X_test_tfidf_grams = cv_grams.transform(X_test_raw)\n",
        "\n",
        "print(X_train_tfidf_grams.shape)\n",
        "print(X_test_tfidf_grams.shape)"
      ],
      "metadata": {
        "id": "ixlW4hUWMBhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = []\n",
        "for x in cv_grams.get_feature_names_out():\n",
        "    if len(x.split(\" \"))>1:\n",
        "        x = x.replace(\" \",\"-\")\n",
        "        vocab.append(x)\n",
        "    else:\n",
        "        vocab.append(x)\n",
        "print(vocab[:10])"
      ],
      "metadata": {
        "id": "92VhfC6wM24I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_grams = MultinomialNB()\n",
        "nb_grams.fit(X_train_tfidf_grams, y_train)\n",
        "y_pred = nb_grams.predict(X_test_tfidf_grams)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Xns3GM3DM85l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_top_words(nb_grams, cv_grams, train_data.target_names, 10)"
      ],
      "metadata": {
        "id": "GwwBZAhBSc-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü• Ejercicios Tarea\n",
        "\n",
        "1. La implementaci√≥n de scikit-learn de Naive Bayes tiene un par√°metro para controlar el suavizado. Haz una busqueda de par√°metros para obtener el *mejor* clasificador Naive Bayes que te sea posible.\n",
        "2. Usando cualquier clasificador cl√°sico de Machine Learning de tu preferencia, busca supera el rendimiento del clasificador Naive Bayes. Escoge uno, ya sea que mejore o el que m√°s se acerque al rendimiento del Naive Bayes.\n",
        "3. Repite el ejercicio 2 usando un clasificador basado en una red neuronal. Reporta el mejor rendimiento.\n",
        "\n",
        "En los tres casos, usa el F1 score. Compara los 3 resultados en un gr√°fico similar al que hicimos en esta notebook.\n",
        "\n",
        "4. Repite los experimentos 1-3 con un nuevo corpus. Este corpus son 50K reviews de IMDB, es una tarea de an√°lisis de sentimientos. El corpus lo puedes bajar usando la instrucci√≥n:\n",
        "\n",
        "        !gdown 18kGdlhOiQNS61wUK7uPbdquKL3XJrgzf\n",
        "\n",
        "5. Con la experiencia ganada con esta notebook y estos experimentos. Responde lo siguiente ¬øqu√© opinas sobre el papel del algoritmo Naive Bayes en la clasificaci√≥n de texto? ¬øqu√© ventajas y desventajas observas?"
      ],
      "metadata": {
        "id": "aT1mTU6z3GZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D355ZXt-TosQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}